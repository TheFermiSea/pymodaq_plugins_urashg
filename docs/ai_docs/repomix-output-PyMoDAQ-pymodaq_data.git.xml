This file is a merged representation of the entire codebase, combined into a single document by Repomix.
The content has been processed where content has been compressed (code blocks are separated by ⋮---- delimiter), security check has been disabled.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Content has been compressed - code blocks are separated by ⋮---- delimiter
- Security check has been disabled - content may contain sensitive information
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.github/
  workflows/
    python-publish.yml
    tests.yml
    updater.yml
src/
  pymodaq_data/
    h5modules/
      exporters/
        base.py
        flimj.py
        hyperspy.py
      __init__.py
      backends.py
      browsing.py
      data_saving.py
      exporter.py
      saving.py
      utils.py
    plotting/
      plotter/
        plotters/
          matplotlib_plotters.py
        plotter.py
    post_treatment/
      __init__.py
      process_to_scalar.py
    __init__.py
    data.py
    numpy_func.py
    slicing.py
tests/
  h5module_test/
    backend_test.py
    data_saving_test.py
    exporter_test.py
    saving_test.py
  data_test.py
  processeor_test.py
  serializer_test.py
.gitattributes
.gitignore
.mu_repo
CITATION.cff
LICENSE
MANIFEST.in
pyproject.toml
README.rst
readthedocs.yml
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".github/workflows/python-publish.yml">
# This workflow will upload a Python Package using Twine when a release is created
# For more information see: https://help.github.com/en/actions/language-and-framework-guides/using-python-with-github-actions#publishing-to-package-registries

name: Upload Python Package

on:
  release:
    types: [created]

jobs:
  build:

    runs-on: ubuntu-latest
          
    steps:
    - uses: actions/checkout@v4.2.2
    - name: Set up Python
      uses: actions/setup-python@v5.6.0
      with:
        python-version: '3.11'
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install hatch hatchling
    - name: Build
      run: hatch build
    - name: publish
      env:
        HATCH_INDEX_USER: ${{ secrets.PYPI_USERNAME }}
        HATCH_INDEX_AUTH: ${{ secrets.PYPI_PASSWORD }}
      run: |
        hatch publish
</file>

<file path=".github/workflows/tests.yml">
name: tests

on:
  workflow_call:
  
  pull_request:

  push:
    branches:
    - '*'
    - '!badges' # to exclude execution if someone pushes on this branch (shouldn't happen)

concurrency:
  # github.workflow: name of the workflow
  # github.event.pull_request.number || github.ref: pull request number or branch name if not a pull request
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  # Cancel in-progress runs when a new workflow with the same group name is triggered
  cancel-in-progress: true
  
jobs:
  tests:
    continue-on-error: true
    strategy:
      fail-fast: false
      matrix:
        os: ["ubuntu-latest", "windows-latest"]
        python-version: ["3.9", "3.10", "3.11", "3.12"]
    runs-on: ${{ matrix.os }}

    steps:     
      # Get the branch name for the badge generation
      - name: Extract branch name
        shell: bash
        run: echo "branch=${GITHUB_REF#refs/heads/}" >> "${GITHUB_OUTPUT}"
        id: extract_branch
        

      - name: Checkout the repo
        uses: actions/checkout@v4.2.2
 
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5.6.0
        with:
          python-version: ${{ matrix.python-version }}
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install flake8 pytest pytest-cov pytest-xdist wheel numpy h5py
          pip install -e .    
      
      # Create folder and set permissions on Ubuntu
      - name: Create local pymodaq folder (Ubuntu)
        if: runner.os == 'Linux'
        run: |
          sudo mkdir -p /etc/.pymodaq
          sudo chmod uo+rw /etc/.pymodaq

      - name: Linting with flake8
        run: |
          # stop the build if there are Python syntax errors or undefined names
          flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics --exclude=docs
      - name: Tests with ${{ matrix.os }} ${{ matrix.python-version }}
        id: tests
        run: |
          mkdir coverage
          pytest --cov=pymodaq_data -n 1
          mv .coverage coverage/coverage_${{ matrix.os }}_${{ matrix.python-version }}
      - name: Upload coverage artifact
        uses: actions/upload-artifact@v4.6.2
        with:
          name: coverage-${{ matrix.os }}-${{ matrix.python-version }}
          path: coverage/coverage_${{ matrix.os }}_${{ matrix.python-version }}


     
      - name: Create destination directory
        if: ${{ always() }}
        run: |
          mkdir -p "${{ steps.extract_branch.outputs.branch }}"

      - name: generate badge (success)
        if: ${{ success() }}
        uses: emibcn/badge-action@v2.0.3
        with:
          label: ''
          status: 'passing'
          color: 'green'
          path: '${{ steps.extract_branch.outputs.branch }}/tests_${{runner.os}}_${{matrix.python-version}}.svg'
      - name: generate badge (fail)
        if: ${{ failure() }}
        uses: emibcn/badge-action@v2.0.3
        with:
          label: ''
          status: 'failing'
          color: 'red'
          path: '${{ steps.extract_branch.outputs.branch }}/tests_${{runner.os}}_${{matrix.python-version}}.svg'


      - name: Upload badge artifact
        if: ${{ always() }}
        uses: actions/upload-artifact@v4.6.2
        with:
          name:  tests_${{runner.os}}_${{matrix.python-version}}
          path: '${{ steps.extract_branch.outputs.branch }}/tests_${{runner.os}}_${{matrix.python-version}}.svg'
          if-no-files-found: error 
    
    outputs:
      branch: ${{ steps.extract_branch.outputs.branch }}
     
  badge-update:
    if: github.repository_owner == 'PyMoDAQ'
    runs-on: ubuntu-latest
    needs: tests # Ensure this job runs after all matrix jobs complete
    steps:
       # switch to badges branches to commit
      - uses: actions/checkout@v4.2.2
        with:
          ref: badges
     
      - name: Download badges
        uses: actions/download-artifact@v4.3.0

      - name: Reorganize badges
        run: |
          rm -rf coverage* || true
          rm -rf $(git ls-files ${{ needs.tests.outputs.branch }}/*) || true
          git rm $(git ls-files ${{ needs.tests.outputs.branch }}/*) || true

          mkdir -p '${{ needs.tests.outputs.branch }}'
          mv tests_*/*.svg '${{ needs.tests.outputs.branch }}'
      - name: Commit badges
        continue-on-error: true
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add ${{ needs.tests.outputs.branch }}
          git commit  --allow-empty -m "Add/Update badge"

      - name: Push badges
        uses: ad-m/github-push-action@v0.8.0
        if: ${{ success() }}
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          branch: badges
  coverage-update:
    runs-on: ubuntu-latest
    needs: tests # Ensure this job runs after all matrix jobs complete

    steps:
      - name: Checkout the repo
        uses: actions/checkout@v4.2.2

      - name: Download all coverage artifacts
        uses: actions/download-artifact@v4.3.0
        with:
          path: ./coverage-reports

      - name: Reorganize reports
        run: |
          cd coverage-reports
          rm -rf tests_*
          for folder in *; do
            mv "${folder}"/* .;
          done;
          rmdir --ignore-fail-on-non-empty * || true
          cd ..
      # We only combine linux reports otherwise the tool complains about windows directories ...
      - name: Combine coverage reports
        run: |
          python -m pip install coverage
          coverage combine ./coverage-reports/coverage_*
          coverage xml -i

      - name: Upload combined coverage report to Codecov
        uses: codecov/codecov-action@v5.4.3
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          files: coverage.xml
</file>

<file path=".github/workflows/updater.yml">
name: GitHub Actions Version Updater

# Controls when the action will run.
on:
  schedule:
    # Automatically run at 00:00 on day-of-month 5.
    - cron:  '0 0 5 * *'

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4.2.2
        with:
          # [Required] Access token with `workflow` scope.
          token: ${{ secrets.WORKFLOW_SECRET }}

      - name: Run GitHub Actions Version Updater
        uses: saadmk11/github-actions-version-updater@v0.8.1
        with:
          # [Required] Access token with `workflow` scope.
          token: ${{ secrets.WORKFLOW_SECRET }}
</file>

<file path="src/pymodaq_data/h5modules/exporters/base.py">
# -*- coding: utf-8 -*-
"""
Created the 02/03/2023

@author: Sebastien Weber
"""
⋮----
# project imports
⋮----
@ExporterFactory.register_exporter()
class H5h5Exporter(H5Exporter)
⋮----
""" Exporter object for saving nodes as single h5 files"""
⋮----
FORMAT_DESCRIPTION = "Single node h5 file"
FORMAT_EXTENSION = "h5"
⋮----
def export_data(self, node: Node, filename: str) -> None
⋮----
"""Export an h5 node"""
# This should allow to get the base file object
⋮----
basefile = node.node._v_file
⋮----
# basefile = node.get_file()
# basefile.copy_file(dstfilename=str(filename), overwrite=False)
⋮----
new_file = H5Backend(backend="tables")
⋮----
@ExporterFactory.register_exporter()
class H5txtExporter(H5Exporter)
⋮----
""" Exporter object for saving nodes as txt files"""
⋮----
FORMAT_DESCRIPTION = "Text files"
FORMAT_EXTENSION = "txt"
⋮----
"""Export the node as a .txt file format"""
⋮----
data = node.read()
⋮----
# in case one has a list of same objects (array of strings for instance, logger or other)
data = np.array(data)
⋮----
data_tot = []
header = []
dtypes = []
fmts = []
⋮----
data = subnode.read()
⋮----
# in case one has a list of same objects (array of strings for instance, logger or other)
⋮----
fmt = '%s'  # for strings
⋮----
fmt = '%d'  # for integers
⋮----
fmt = '%.6f'  # for decimal numbers
⋮----
data_trans = np.array(list(zip(*data_tot)), dtype=dtypes)
⋮----
@ExporterFactory.register_exporter()
class H5npyExporter(H5Exporter)
⋮----
""" Exporter object for saving nodes as npy files"""
⋮----
FORMAT_DESCRIPTION = "Binary NumPy format"
FORMAT_EXTENSION = "npy"
⋮----
"""Export the node as a numpy binary file format"""
# String __contain__ method will evaluate to True for CARRAY,EARRAY,VLARRAY,stringARRAY
</file>

<file path="src/pymodaq_data/h5modules/exporters/flimj.py">
# -*- coding: utf-8 -*-
"""
Created the 02/03/2023

@author: Sebastien Weber
"""
⋮----
# project imports
⋮----
@ExporterFactory.register_exporter()
class H5asciiExporter(H5Exporter)
⋮----
""" Exporter object for saving nodes as txt files"""
⋮----
FORMAT_DESCRIPTION = "Ascii flimj file"
FORMAT_EXTENSION = "ascii"
⋮----
def export_data(self, node: Node, filename: str) -> None
⋮----
data = node.read()
⋮----
# in case one has a list of same objects (array of strings for instance, logger or other)
data = np.array(data)
⋮----
data_tot = []
header = []
dtypes = []
fmts = []
⋮----
data = subnode.read()
⋮----
# in case one has a list of same objects (array of strings for instance, logger or other)
⋮----
fmt = '%s'  # for strings
⋮----
fmt = '%d'  # for integers
⋮----
fmt = '%.6f'  # for decimal numbers
⋮----
data_trans = np.array(list(zip(*data_tot)), dtype=dtypes)
</file>

<file path="src/pymodaq_data/h5modules/exporters/hyperspy.py">
# -*- coding: utf-8 -*-
"""
Created the 02/03/2023

@author: Nicolas Tappy
"""
# Standard imports
⋮----
# 3rd party imports
⋮----
# project imports
⋮----
# This is needed to log
logger = set_logger(get_module_name(__file__))
⋮----
@ExporterFactory.register_exporter()
    class H5hspyExporter(H5Exporter)
⋮----
""" Exporter object for saving nodes as hspy files"""
⋮----
FORMAT_DESCRIPTION = "Hyperspy file format"
FORMAT_EXTENSION = "hspy"
⋮----
def export_data(self, node: Node, filename) -> None
⋮----
"""Exporting a .h5 node as a hyperspy object"""
⋮----
# first verify if the node type is compatible with export. Only data nodes are.
############## compatible with v4 done by Seb
nodetype = node.attrs['data_type']
⋮----
# If yes we can use this function (the same for plotting in h5browser) to extract information
⋮----
data_loader = DataLoader(node.to_h5_backend())
⋮----
#use the method in data_saving below to load properly data from nodes. The data object is a DataWithAxes
#of dimensionality DataND
#you can get all axes (nav and signals) from data.axes
#each axis as an index corresponding to the data shape
# data.nav_index is a tuple containing the index of the nav axes
⋮----
data = data_loader.load_data(node, with_bkg=False)
axes = data.axes #all axes
nav_axes = data.get_nav_axes_with_data() #only navigation one
⋮----
#deprecated
# data, axes, nav_axes, is_spread = get_h5_data_from_node(node)
is_spread = node.attrs['distribution'] != 'uniform'
⋮----
data_array = data.data.reshape(data.shape, order='F')
⋮----
#################################### TODO below to be done by Nicolas :-) and should be tested!!
⋮----
# get_h5_data_from_node will return sometimes empty axes that we must yeet.
axes_keys_to_remove = []
⋮----
# Else just raise an error.
⋮----
# Verify that this is not an adaptive scan. If it is we throw and say it is not supported.
⋮----
# Then we build the hyperspy axes objects.
hyperspy_axes = []
⋮----
unique_ax = extract_axis(ax)
⋮----
indexv = data.shape.index(len(unique_ax))
⋮----
is_nav = key.startswith('nav')
⋮----
ordered_axes = sorted(hyperspy_axes, key=lambda d: d['index_in_array'])
⋮----
# Then we build the hyperspy object. First we must know its dimensionality
# from the number of signal axes.
dim = len(axes) - len(nav_axes)
⋮----
# Then signal1D
sig = hs.signals.Signal1D(data=data, original_metadata={}, axes=ordered_axes)
⋮----
# Then signal2D
sig = hs.signals.Signal2D(data=data, original_metadata={}, axes=ordered_axes)
⋮----
# Then basesignal
sig = hs.signals.BaseSignal(data=data, original_metadata={}, axes=ordered_axes)
⋮----
# Finally save
⋮----
"""Build an axis based on the input data. Choose between a UniformDataAxis or
            DataAxis object based on a quick linearity check of the input data."""
⋮----
axis_dict = {'_type': 'UniformDataAxis',
⋮----
axis_dict = {'_type': 'DataAxis',
⋮----
def build_hyperspy_original_metadata(self, node: Node)
⋮----
"""Build original metadata dictionary"""
</file>

<file path="src/pymodaq_data/h5modules/__init__.py">

</file>

<file path="src/pymodaq_data/h5modules/backends.py">
# -*- coding: utf-8 -*-
"""
Created the 15/11/2022

@author: Sebastien Weber
"""
⋮----
config = Config()
logger = set_logger(get_module_name(__file__))
⋮----
backends_available = []
⋮----
# default backend
is_tables = True
⋮----
except Exception as e:                              # pragma: no cover
⋮----
is_tables = False
⋮----
is_h5py = True
# other possibility
⋮----
is_h5py = False
⋮----
is_h5pyd = True
# this one is to be used for remote reading/writing towards a HSDS server (or h5serv), see HDFGroup
⋮----
is_h5pyd = False
⋮----
class NodeError(Exception)
⋮----
class SaveType(BaseEnum)
⋮----
scan = 0
detector = 1
logger = 2
custom = 3
actuator = 4
optimizer = 5
⋮----
class GroupType(BaseEnum)
⋮----
detector = 0
actuator = 1
data = 2
ch = 3
scan = 4
external_h5 = 5
data_dim = 6
data_logger = 7
⋮----
class InvalidExport(Exception)
⋮----
def check_mandatory_attrs(attr_name, attr)
⋮----
"""for cross compatibility between different backends. If these attributes have binary value, then decode them

    Parameters
    ----------
    attr_name
    attr

    Returns
    -------

    """
⋮----
def get_attr(node, attr_name, backend='tables')
⋮----
attr = node._v_attrs[attr_name]
attr = check_mandatory_attrs(attr_name, attr)
⋮----
attrs = dict([])
⋮----
attrval = node._v_attrs[attr_name]
attrval = check_mandatory_attrs(attr_name, attrval)
⋮----
attr = node.attrs[attr_name]
⋮----
attrval = node.attrs[attr_name]
⋮----
def set_attr(node, attr_name, attr_value, backend='tables')
⋮----
class InvalidGroupType(Exception)
⋮----
class InvalidSave(Exception)
⋮----
class InvalidGroupDataType(Exception)
⋮----
class InvalidDataType(Exception)
⋮----
class InvalidDataDimension(Exception)
⋮----
class InvalidScanType(Exception)
⋮----
class Node(object)
⋮----
def __init__(self, node, backend)
⋮----
if isinstance(node, Node):  # to ovoid recursion if one call Node(Node()) or even more
⋮----
def __str__(self)
⋮----
# Get this class name
classname = self.__class__.__name__
# The title
title = self.attrs['TITLE']
⋮----
@property
    def node(self)
⋮----
def __eq__(self, other)
⋮----
@property
    def parent_node(self) -> 'GROUP'
⋮----
mod = importlib.import_module('.backends', 'pymodaq_data.h5modules')
⋮----
p = self.node._v_parent
⋮----
p = self.node.parent
klass = get_attr(p, 'CLASS', self.backend)
_cls = getattr(mod, klass)
⋮----
@property
    def h5file(self)
⋮----
def to_h5_backend(self) -> 'H5Backend'
⋮----
h5_backend = H5Backend(self.backend)
⋮----
def set_attr(self, key, value)
⋮----
def get_attr(self, item)
⋮----
@property
    def attrs(self)
⋮----
@property
    def name(self)
⋮----
"""return node name
        """
⋮----
path = self._node.name
⋮----
@property
    def title(self)
⋮----
@property
    def path(self)
⋮----
"""return node path
        Parameters
        ----------
        node (str or node instance), see h5py and pytables documentation on nodes

        Returns
        -------
        str : full path of the node
        """
⋮----
class GROUP(Node)
⋮----
"""Return a short string representation of the group.
        """
⋮----
pathname = self.path
⋮----
def __repr__(self)
⋮----
"""Return a detailed string representation of the group.
        """
⋮----
rep = [
childlist = '[%s]' % (', '.join(rep))
⋮----
def children(self) -> Dict[str, Node]
⋮----
"""Get a dict containing all children node hanging from self whith their name as keys

        Returns
        -------
        dict: keys are children node names, values are the children nodes

        See Also
        --------
        children_name
        """
⋮----
children = dict([])
⋮----
klass = get_attr(child, 'CLASS', self.backend)
⋮----
_cls = GROUP
⋮----
def get_child(self, name: str) -> Node
⋮----
def children_name(self)
⋮----
"""Gets the sorted list of children name hanging from self

        Returns
        -------
        list: list of name of the children
        """
⋮----
def remove_children(self)
⋮----
children_dict = self.children()
⋮----
class CARRAY(Node)
⋮----
@property
    def array(self)
⋮----
"""This provides more metainfo in addition to standard __str__"""
⋮----
def __getitem__(self, item)
⋮----
def __setitem__(self, key, value)
⋮----
def read(self)
⋮----
def __len__(self)
⋮----
class EARRAY(CARRAY)
⋮----
def __init__(self, array, backend)
⋮----
def append(self, data: np.ndarray, expand=True)
⋮----
""" appends a ndarray after the current data in the enlargeable array

        Considering the shape length of the enlargeable array is n+1

        The data to append could be:

        * a single element (without the enlargeable shape index that is always the first
        index, that is of shape length n). In that case the first index of the enlargeable array
        is increased by one.
        * an ensemble of elements (a ndarray) of shape length of (n+1).

        Parameters
        ----------
        data: np.ndarray
            the data array to append to the enlargeable node
        expand: bool
            If True the data array will have its shape expanded by one dim

        """
⋮----
shape = [1]
⋮----
data = data.reshape(shape)
extended_first_index = 1
⋮----
extended_first_index = data.shape[0]
⋮----
data = np.expand_dims(data, 1)
⋮----
sh = list(self.attrs['shape'])
⋮----
def append_backend(self, data)
⋮----
class VLARRAY(EARRAY)
⋮----
def append(self, data)
⋮----
class StringARRAY(VLARRAY)
⋮----
data_list = super().read()
⋮----
def append(self, string)
⋮----
data = self.string_to_array(string)
⋮----
def array_to_string(self, array)
⋮----
def string_to_array(self, string)
⋮----
class Attributes(object)
⋮----
def __init__(self, node, backend='tables')
⋮----
item = item.upper()
attr = get_attr(self._node.node, item, backend=self.backend)
# if isinstance(attr, bytes):
#    attr = attr.decode()
⋮----
key = key.upper()
⋮----
def __iter__(self)
⋮----
def __next__(self)
⋮----
def to_dict(self) -> dict
⋮----
"""Returns attributes name/value as a dict"""
attrs_dict = dict()
⋮----
@property
    def attrs_name(self)
⋮----
"""The string representation for this object."""
⋮----
# The pathname
⋮----
pathname = self._node.node._v_pathname
⋮----
pathname = self._node.node.name
⋮----
# The attribute names
attrnumber = len([n for n in self.attrs_name])
⋮----
attrnames = self.attrs_name
⋮----
rep = ['%s := %s' % (attr, str(self[attr]))
attrlist = '[%s]' % (',\n    '.join(rep))
⋮----
class H5Backend
⋮----
def __init__(self, backend='tables')
⋮----
@h5file.setter
    def h5file(self, file)
⋮----
@property
    def filename(self)
⋮----
def isopen(self)
⋮----
def close_file(self)
⋮----
"""Flush data and close the h5file
        """
⋮----
print(e)  # no big deal
⋮----
def open_file(self, fullpathname, mode='r', title='PyMoDAQ file', **kwargs)
⋮----
def save_file_as(self, filenamepath='h5copy.txt')
⋮----
def root(self)
⋮----
def get_attr(self, node, attr_name=None)
⋮----
node = node.node
⋮----
def set_attr(self, node, attr_name, attr_value)
⋮----
def has_attr(self, node, attr_name)
⋮----
def flush(self)
⋮----
def define_compression(self, compression, compression_opts)
⋮----
"""Define cmpression library and level of compression
        Parameters
        ----------
        compression: (str) either gzip and zlib are supported here as they are compatible
                        but zlib is used by pytables while gzip is used by h5py
        compression_opts (int) : 0 to 9  0: None, 9: maximum compression
        """
#
⋮----
compression = 'zlib'
⋮----
compression = 'gzip'
⋮----
def get_set_group(self, where, name, title='', **kwargs)
⋮----
"""Retrieve or create (if absent) a node group
        Get attributed to the class attribute ``current_group``

        Parameters
        ----------
        where: str or node
               path or parent node instance
        name: str
              group node name
        title: str
               node title

        Keyword Arguments:
            any other metadata related to this node (for example: origin)
        Returns
        -------
        group: group node
        """
⋮----
where = where.node
⋮----
group = self._h5file.create_group(where, name, title)
⋮----
group = self.get_node(where).node.create_group(name)
⋮----
group = self.get_node(where, name)
⋮----
def get_group_by_title(self, where, title)
⋮----
node = self.get_node(where).node
⋮----
child = node[child_name]
⋮----
def is_node_in_group(self, where, name)
⋮----
"""
        Check if a given node with name is in the group defined by where (comparison on lower case strings)
        Parameters
        ----------
        where: (str or node)
                path or parent node instance
        name: (str)
              group node name

        Returns
        -------
        bool
            True if node exists, False otherwise
        """
⋮----
def get_node(self, where, name=None) -> Node
⋮----
node = self._h5file.get_node(where, name)
⋮----
node = self._h5file.get(where)
⋮----
where = where.get(name)
node = where
⋮----
attr = self.get_attr(node, 'CLASS')
⋮----
def get_node_name(self, node)
⋮----
"""return node name
        Parameters
        ----------
        node (str or node instance), see h5py and pytables documentation on nodes

        Returns
        -------
        str: name of the node
        """
⋮----
def get_node_path(self, node)
⋮----
def get_parent_node(self, node)
⋮----
def get_children(self, where)
⋮----
"""Get a dict containing all children node hanging from where with their name as keys and types among Node,
        CARRAY, EARRAY, VLARRAY or StringARRAY

        Parameters
        ----------
        where (str or node instance)
            see h5py and pytables documentation on nodes, and Node objects of this module

        Returns
        -------
        dict: keys are children node names, values are the children nodes

        See Also
        --------
        :meth:`.GROUP.children_name`

        """
where = self.get_node(where)  # return a node object in case where is a string
⋮----
def walk_nodes(self, where)
⋮----
def walk_groups(self, where)
⋮----
stack = [where]
⋮----
obj = stack.pop()
children = [child for child in self.get_children(obj).values() if child.attrs['CLASS'] == 'GROUP']
⋮----
def read(self, array, *args, **kwargs)
⋮----
array = array.array
⋮----
def create_carray(self, where, name, obj=None, title='')
⋮----
dtype = obj.dtype
⋮----
array = CARRAY(self._h5file.create_carray(where, name, obj=obj,
⋮----
array = CARRAY(self.get_node(where).node.create_dataset(name, data=obj, **self.compression),
⋮----
array = CARRAY(self.get_node(where).node.create_dataset(name, data=obj), self.backend)
⋮----
'CLASS'] = 'CARRAY'  # direct writing using h5py to be compatible with pytable automatic class writing as binary
⋮----
def create_earray(self, where, name, dtype, data_shape=None, title='')
⋮----
"""create enlargeable arrays from data with a given shape and of a given type. The array is enlargeable along
        the first dimension
        """
⋮----
dtype = np.dtype(dtype)
shape = [0]
⋮----
shape = tuple(shape)
⋮----
atom = self.h5_library.Atom.from_dtype(dtype)
array = EARRAY(self._h5file.create_earray(where, name, atom, shape=shape, title=title,
⋮----
maxshape = [None]
⋮----
maxshape = tuple(maxshape)
⋮----
array = EARRAY(
⋮----
'CLASS'] = 'EARRAY'  # direct writing using h5py to be compatible with pytable automatic class writing as binary
⋮----
def create_vlarray(self, where, name, dtype, title='')
⋮----
"""create variable data length and type and enlargeable 1D arrays

        Parameters
        ----------
        where: (str) group location in the file where to create the array node
        name: (str) name of the array
        dtype: (dtype) numpy dtype style, for particular case of strings, use dtype='string'
        title: (str) node title attribute (written in capitals)

        Returns
        -------
        array

        """
⋮----
dtype = np.dtype(np.uint8)
subdtype = 'string'
⋮----
subdtype = ''
⋮----
array = StringARRAY(self._h5file.create_vlarray(where, name, atom, title=title,
⋮----
array = VLARRAY(self._h5file.create_vlarray(where, name, atom, title=title,
⋮----
maxshape = (None,)
⋮----
dt = self.h5_library.vlen_dtype(dtype)
⋮----
dt = h5pyd.special_dtype(dtype)
⋮----
array = StringARRAY(self.get_node(where).node.create_dataset(name, (0,), dtype=dt,
⋮----
array = VLARRAY(self.get_node(where).node.create_dataset(name, (0,), dtype=dt, **self.compression,
⋮----
array = VLARRAY(self.get_node(where).node.create_dataset(name, (0,), dtype=dt,
⋮----
'CLASS'] = 'VLARRAY'  # direct writing using h5py to be compatible with pytable automatic class writing as binary
⋮----
def add_group(self, group_name, group_type: Union[GroupType, str], where, title='', metadata=dict([])) -> GROUP
⋮----
"""
        Add a node in the h5 file tree of the group type
        Parameters
        ----------
        group_name: (str) a custom name for this group
        group_type: str or GroupType enum
            one of the possible values of GroupType, should be enforced by higher level modules not here
        where: (str or node) parent node where to create the new group
        metadata: (dict) extra metadata to be saved with this new group node

        Returns
        -------
        (node): newly created group node
        """
⋮----
group_type = group_type.name
⋮----
node = self.get_node(where, group_name)
⋮----
node = self.get_set_group(where, utils.capitalize(group_name), title)
</file>

<file path="src/pymodaq_data/h5modules/browsing.py">
# -*- coding: utf-8 -*-
"""
Created the 15/11/2022

@author: Sebastien Weber
"""
⋮----
config = Config()
logger = set_logger(get_module_name(__file__))
⋮----
class H5BrowserUtil(H5Backend)
⋮----
"""Utility object to interact and get info and data from a hdf5 file

    Inherits H5Backend and all its functionalities

    Parameters
    ----------
    backend: str
        The used hdf5 backend: either tables, h5py or h5pyd
    """
def __init__(self, backend='tables')
⋮----
def export_data(self, node_path='/', filesavename: str = 'datafile.h5', filter=None)
⋮----
"""Initialize the correct exporter and export the node"""
⋮----
# Format the node and file type
filepath = Path(filesavename)
node = self.get_node(node_path)
# Separate dot from extension
extension = filepath.suffix[1:]
# Obtain the suitable exporter object
exporter = ExporterFactory.create_exporter(
# Export the data
⋮----
def get_h5file_scans(self, where='/')
⋮----
"""Get the list of the scan nodes in the file

        Parameters
        ----------
        where: str
            the path in the file

        Returns
        -------
        list of dict
            dict with keys: scan_name, path (within the file) and data (the live scan png image)
        """
# TODO add a test for this method
scan_list = []
where = self.get_node(where)
⋮----
def get_h5_attributes(self, node_path)
⋮----
"""
        """
⋮----
attrs_names = node.attrs.attrs_name
attr_dict = OrderedDict(node.attrs.to_dict())
⋮----
settings = None
scan_settings = None
⋮----
settings = node.attrs['settings']
⋮----
scan_settings = node.attrs['scan_settings']
pixmaps = []
</file>

<file path="src/pymodaq_data/h5modules/data_saving.py">
# -*- coding: utf-8 -*-
"""
Created the 21/11/2022

@author: Sebastien Weber
"""
⋮----
SPECIAL_GROUP_NAMES = dict(nav_axes='NavAxes')
⋮----
class AxisError(Exception)
⋮----
class DataManagement(metaclass=ABCMeta)
⋮----
"""Base abstract class to be used for all specialized object saving and loading data to/from a h5file

    Attributes
    ----------
    data_type: DataType
        The enum for this type of data, here abstract and should be redefined
    """
data_type: DataType = abstract_attribute()
_h5saver: H5SaverLowLevel = abstract_attribute()
⋮----
@classmethod
    def _format_node_name(cls, ind: int) -> str
⋮----
""" Format the saved node following the data_type attribute and an integer index

        Parameters
        ----------
        ind: int

        Returns
        -------
        str: the future name of the node
        """
⋮----
def __enter__(self)
⋮----
def __exit__(self, exc_type, exc_val, exc_tb)
⋮----
def close_file(self)
⋮----
def close(self)
⋮----
def _get_next_node_name(self, where: Union[str, Node]) -> str
⋮----
"""Get the formatted next node name given the ones already saved

        Parameters
        ----------
        where: Union[Node, str]
            the path of a given node or the node itself

        Returns
        -------
        str: the future name of the node
        """
⋮----
def get_last_node_name(self, where: Union[str, Node]) -> Union[str, None]
⋮----
"""Get the last node name among the ones already saved

        Parameters
        ----------
        where: Union[Node, str]
            the path of a given node or the node itself

        Returns
        -------
        str: the name of the last saved node or None if none saved
        """
index = self._get_next_data_type_index_in_group(where) - 1
⋮----
def get_node_from_index(self, where: Union[str, Node], index: int) -> Node
⋮----
def get_index_from_node_name(self, where: Union[str, Node])
⋮----
node = self._h5saver.get_node(where)
⋮----
index = int(node.name.split(self.data_type.value)[1])
⋮----
def _get_next_data_type_index_in_group(self, where: Union[Node, str]) -> int
⋮----
"""Check how much node with a given data_type are already present within the GROUP where
        Parameters
        ----------
        where: Union[Node, str]
            the path of a given node or the node itself

        Returns
        -------
        int: the next available integer to index the node name
        """
ind = 0
⋮----
def _is_node_of_data_type(self, where: Union[str, Node]) -> bool
⋮----
"""Check if a given node is of the data_type of the real class implementation
        
        eg 'axis' for the AxisSaverLoader
        
        Parameters
        ----------
        where: Union[Node, str]
            the path of a given node or the node itself

        Returns
        -------
        bool
        """
node = self._get_node(where)
⋮----
def _get_node(self, where: Union[str, Node]) -> Node
⋮----
"""Utility method to get a node from a node or a string"""
⋮----
def _get_nodes(self, where: Union[str, Node]) -> List[Node]
⋮----
"""Get Nodes hanging from where including where

        Parameters
        ----------
        where: Union[Node, str]
            the path of a given node or the node itself

        Returns
        -------
        List[Node]
        """
⋮----
def _get_nodes_from_data_type(self, where: Union[str, Node]) -> List[Node]
⋮----
"""Get the node list hanging from a parent and having the same data type as self
        
        Parameters
        ----------
        where: Union[Node, str]
            the path of a given node

        Returns
        -------
        list of Nodes
        """
⋮----
parent_node = node
⋮----
parent_node = node.parent_node
⋮----
nodes = []
⋮----
class AxisSaverLoader(DataManagement)
⋮----
"""Specialized Object to save and load Axis object to and from a h5file

    Parameters
    ----------
    h5saver: H5SaverLowLevel

    Attributes
    ----------
    data_type: DataType
        The enum for this type of data, here 'axis'
    """
data_type = DataType['axis']
⋮----
def __init__(self, h5saver: H5SaverLowLevel)
⋮----
def add_axis(self, where: Union[Node, str], axis: Axis, enlargeable=False)
⋮----
"""Write Axis info at a given position within a h5 file

        Parameters
        ----------
        where: Union[Node, str]
            the path of a given node or the node itself
        axis: Axis
            the Axis object to add as a node in the h5file
        enlargeable: bool
            Specify if the underlying array will be enlargebale
        """
array = self._h5saver.add_array(where, self._get_next_node_name(where), self.data_type, title=axis.label,
⋮----
def load_axis(self, where: Union[Node, str]) -> Axis
⋮----
"""create an Axis object from the data and metadata at a given node if of data_type: 'axis

        Parameters
        ----------
        where: Union[Node, str]
            the path of a given node or the node itself

        Returns
        -------
        Axis
        """
axis_node = self._get_node(where)
⋮----
def get_axes(self, where: Union[Node, str]) -> List[Axis]
⋮----
"""Return a list of Axis objects from the Axis Nodes hanging from (or among) a given Node

        Parameters
        ----------
        where: Union[Node, str]
            the path of a given node or the node itself

        Returns
        -------
        List[Axis]: the list of all Axis object
        """
axes = []
⋮----
axis = self.load_axis(node)
# if axis.size > 1:
#     axes.append(axis)
⋮----
class DataSaverLoader(DataManagement)
⋮----
"""Specialized Object to save and load DataWithAxes object to and from a h5file

    Parameters
    ----------
    h5saver: H5SaverLowLevel or Path or str

    Attributes
    ----------
    data_type: DataType
        The enum for this type of data, here 'data'
    """
data_type = DataType['data']
⋮----
def __init__(self, h5saver: Union[H5SaverLowLevel, Path])
⋮----
h5saver_tmp = H5SaverLowLevel()
⋮----
h5saver = h5saver_tmp
⋮----
def isopen(self) -> bool
⋮----
""" Get the opened status of the underlying hdf5 file"""
⋮----
def add_data(self, where: Union[Node, str], data: DataWithAxes, save_axes=True, **kwargs)
⋮----
"""Adds Array nodes to a given location adding eventually axes as others nodes and metadata

        Parameters
        ----------
        where: Union[Node, str]
            the path of a given node or the node itself
        data: DataWithAxes
        save_axes: bool
        """
⋮----
metadata = dict(timestamp=data.timestamp, label=data.labels[ind_data],
⋮----
"""

        Parameters
        ----------
        where: Union[Node, str]
            the path of a given node or the node itself

        Returns
        -------

        """
⋮----
def get_bkg_nodes(self, where: Union[Node, str])
⋮----
bkg_nodes = []
⋮----
"""

        Parameters
        ----------
        where: Union[Node, str]
            the path of a given node or the node itself
        with_bkg: bool
            If True try to load background node and return the array with background subtraction
        load_all: bool
            If True load all similar nodes hanging from a parent

        Returns
        -------
        list of ndarray
        """
where = self._get_node(where)
⋮----
bkg_nodes = self.get_bkg_nodes(where.parent_node)
⋮----
with_bkg = False
⋮----
getter = self._get_nodes_from_data_type
⋮----
getter = self._get_nodes
⋮----
def _get_signal_indexes_to_squeeze(self, array: Union[CARRAY, EARRAY])
⋮----
""" Get the tuple of indexes in the array shape that are not navigation and should be
        squeezed"""
sig_indexes = []
⋮----
def load_data(self, where, with_bkg=False, load_all=False) -> DataWithAxes
⋮----
"""Return a DataWithAxes object from the Data and Axis Nodes hanging from (or among) a
        given Node

        Does not include navigation axes stored elsewhere in the h5file. The node path is stored in
        the DatWithAxis using the attribute path

        Parameters
        ----------
        where: Union[Node, str]
            the path of a given node or the node itself
        with_bkg: bool
            If True try to load background node and return the data with background subtraction
        load_all: bool
            If True, will load all data hanging from the same parent node

        See Also
        --------
        load_data
        """
⋮----
data_node = self._get_node(where)
⋮----
parent_node = data_node.parent_node
data_nodes = self._get_nodes_from_data_type(parent_node)
data_node = data_nodes[0]
error_node = data_node
⋮----
data_nodes = [data_node]
error_node = None
⋮----
error_node_index = self.get_index_from_node_name(data_node)
⋮----
error_node = self._error_saver.get_node_from_index(parent_node, error_node_index)
⋮----
ndarrays = [squeeze(data_node.read()) for data_node in data_nodes]
axes = [Axis(label=data_node.attrs['label'], units=data_node.attrs['units'],
error_arrays = None
⋮----
ndarrays = self.get_data_arrays(data_node, with_bkg=with_bkg, load_all=load_all)
axes = self.get_axes(parent_node)
⋮----
error_arrays = self._error_saver.get_data_arrays(error_node, load_all=load_all)
⋮----
extra_attributes = data_node.attrs.to_dict()
⋮----
data = DataWithAxes(data_node.attrs['TITLE'],
⋮----
class BkgSaver(DataSaverLoader)
⋮----
"""Specialized Object to save and load DataWithAxes background object to and from a h5file

    Parameters
    ----------
    hsaver: H5SaverLowLevel

    Attributes
    ----------
    data_type: DataType
        The enum for this type of data, here 'bkg'
    """
data_type = DataType['bkg']
⋮----
class ErrorSaverLoader(DataSaverLoader)
⋮----
"""Specialized Object to save and load DataWithAxes errors bars to and from a h5file

    Parameters
    ----------
    hsaver: H5SaverLowLevel

    Attributes
    ----------
    data_type: DataType
        The enum for this type of data, here 'error'
    """
data_type = DataType['error']
⋮----
class DataEnlargeableSaver(DataSaverLoader)
⋮----
""" Specialized Object to save and load enlargeable DataWithAxes saved object to and from a
    h5file

    Particular case of DataND with a single *nav_indexes* parameter will be appended as chunks
    of signal data

    Parameters
    ----------
    h5saver: H5SaverLowLevel

    Attributes
    ----------
    data_type: DataType
        The enum for this type of data, here 'data_enlargeable'

    Notes
    -----
    To be used to save data from a timed logger (DAQViewer continuous saving or DAQLogger extension) or from an
    adaptive scan where the final shape is unknown or other module that need this feature
    """
data_type = DataType['data_enlargeable']
⋮----
""" Create enlargeable array to store data

        Parameters
        ----------
        where: Union[Node, str]
            the path of a given node or the node itself
        data: DataWithAxes
        save_axes: bool
            if True, will save signal axes as data nodes
        add_enl_axes: bool
            if True, will save enlargeable axes as data nodes (depending on the self._enl_axis_names
            field)

        Notes
        -----
        Because data will be saved at a given index in the enlargeable array, related signal axes
        will have their index increased by 1)
        """
⋮----
nav_indexes = list(data.nav_indexes)
nav_indexes = ([0] +
⋮----
axis = axis.copy()
axis.index += 1  # because of enlargeable data will have an extra shape
⋮----
""" Append data to an enlargeable array node

        Data of dim (0, 1 or 2) will be just appended to the enlargeable array.

        Uniform DataND with one navigation axis of length (Lnav) will be considered as a collection
        of Lnav signal data of dim (0, 1 or 2) and will therefore be appended as Lnav signal data

        Parameters
        ----------
        where: Union[Node, str]
            the path of a given node or the node itself
        data: DataWithAxes
        axis_values: optional, list of floats
            the new spread axis values added to the data
            if None the axes are not added to the h5 file


        """
add_enl_axes = axis_values is not None
⋮----
data_init = data
elif len(data.nav_indexes) == 1:  # special case of DataND data
data_init = data.inav[0]
add_enl_axes = True
axis = data.get_axis_from_index(data.nav_indexes[0])[0]
axis_values = (axis.get_data(),)
⋮----
elif len(data.nav_indexes) == 1:  # special case of DataND data
⋮----
array: EARRAY = self.get_node_from_index(where, ind_data)
⋮----
axis_array: EARRAY = self._axis_saver.get_node_from_index(where, ind_axis)
⋮----
class DataExtendedSaver(DataSaverLoader)
⋮----
"""Specialized Object to save and load DataWithAxes saved object to and from a h5file in extended arrays

    Parameters
    ----------
    h5saver: H5SaverLowLevel
    extended_shape: Tuple[int]
        the extra shape compared to the data the h5array will have

    Attributes
    ----------
    data_type: DataType
        The enum for this type of data, here 'data'
    """
⋮----
def __init__(self, h5saver: H5SaverLowLevel, extended_shape: Tuple[int])
⋮----
""" Create array with extra dimensions (from scan) to store data

        Parameters
        ----------
        where: Union[Node, str]
            the path of a given node or the node itself
        data: DataWithAxes
        save_axes: bool

        Notes
        -----
        Because data will be saved at a given index in the "scan" array, related axes will have their index
        increased by the length of the scan dim (1 for scan1D, 2 for scan2D, ...)
        """
⋮----
nav_indexes = [ind for ind in range(len(self.extended_shape))] +\
⋮----
# because there will be len(self.extended_shape) extra navigation axes
⋮----
"""Adds given DataWithAxes at a location within the initialized h5 array

        Parameters
        ----------
        where: Union[Node, str]
            the path of a given node or the node itself
        data: DataWithAxes
        indexes: Iterable[int]
            indexes where to save data in the init h5array (should have the same length as extended_shape and with values
            coherent with this shape
        """
⋮----
#todo check that getting with index is safe...
array: CARRAY = self.get_node_from_index(where, ind_data)
⋮----
# maybe use array.__setitem__(indexes, data[ind_data]) if it's not working
⋮----
class DataToExportSaver
⋮----
"""Object used to save DataToExport object into a h5file following the PyMoDAQ convention

    Parameters
    ----------
    h5saver: H5SaverLowLevel

    """
def __init__(self, h5saver: Union[H5SaverLowLevel, Path, str], save_type=SaveType.scan)
⋮----
h5saver_tmp = H5SaverLowLevel(save_type=save_type)
⋮----
def _get_node(self, where: Union[Node, str]) -> Node
⋮----
@staticmethod
    def channel_formatter(ind: int)
⋮----
"""All DataWithAxes included in the DataToExport will be saved into a channel group indexed
        and formatted as below"""
⋮----
def add_data(self, where: Union[Node, str], data: DataToExport, settings_as_xml='', **kwargs)
⋮----
"""

        Parameters
        ----------
        where: Union[Node, str]
            the path of a given node or the node itself
        data: DataToExport
        settings_as_xml: str
            The settings parameter as an XML string
        Keyword Arguments:
            all extra metadata to be saved in the group node where data will be saved

        """
⋮----
dims = data.get_dim_presents()
⋮----
dim_group = self._h5saver.get_set_group(where, dim)
⋮----
# dwa: DataWithAxes filtered by dim
dwa_group = self._h5saver.get_set_group(dim_group, self.channel_formatter(ind),
# dwa_group = self._h5saver.add_ch_group(dim_group, dwa.name)
⋮----
def add_bkg(self, where: Union[Node, str], data: DataToExport)
⋮----
dwa_group = self._h5saver.get_set_group(dim_group,
# dwa_group = self._get_node_from_title(dim_group, dwa.name)
⋮----
def add_error(self, where: Union[Node, str], data: DataToExport)
⋮----
class DataToExportEnlargeableSaver(DataToExportSaver)
⋮----
"""Generic object to save DataToExport objects in an enlargeable h5 array

    The next enlarged value should be specified in the add_data method

    Parameters
    ----------
    h5saver: H5SaverLowLevel
    enl_axis_names: Iterable[str]
        The names of the enlargeable axis, default ['nav_axis']
    enl_axis_units: Iterable[str]
        The names of the enlargeable axis, default ['']
    axis_name: str, deprecated use enl_axis_names
        the name of the enlarged axis array
    axis_units: str, deprecated use enl_axis_units
        the units of the enlarged axis array
    """
⋮----
if enl_axis_names is None:  # for backcompatibility
enl_axis_names = (axis_name,)
if enl_axis_units is None:  # for backcompatibilitu
enl_axis_units = (axis_units,)
⋮----
"""

        Parameters
        ----------
        where: Union[Node, str]
            the path of a given node or the node itself
        data: DataToExport
            The data to be saved into an enlargeable array
        axis_values: iterable float or np.ndarray
            The next value (or values) of the enlarged axis
        axis_value: float or np.ndarray #deprecated in 4.2.0, use axis_values
            The next value (or values) of the enlarged axis
        settings_as_xml: str
            The settings parameter as an XML string
        Keyword Arguments:
            all extra metadata to be saved in the group node where data will be saved
        """
⋮----
axis_values = [axis_value]
⋮----
# a parent navigation group (same for all data nodes)
⋮----
nav_group = self._h5saver.get_set_group(where, SPECIAL_GROUP_NAMES['nav_axes'])
⋮----
axis = Axis(label=self._enl_axis_names[ind],
axis_array = self._nav_axis_saver.add_axis(nav_group, axis, enlargeable=True)
⋮----
axis_array: EARRAY = self._nav_axis_saver.get_node_from_index(nav_group, ind)
⋮----
class DataToExportTimedSaver(DataToExportEnlargeableSaver)
⋮----
"""Specialized DataToExportEnlargeableSaver to save data as a function of a time axis

    Only one element ca be added at a time, the time axis value are enlarged using the data to be
    added timestamp

    Notes
    -----
    This object is made for continuous saving mode of DAQViewer and logging to h5file for DAQLogger
    """
⋮----
class DataToExportExtendedSaver(DataToExportSaver)
⋮----
"""Object to save DataToExport at given indexes within arrays including extended shape

    Mostly used for data generated from the DAQScan

    Parameters
    ----------
    h5saver: H5SaverLowLevel
    extended_shape: Tuple[int]
        the extra shape compared to the data the h5array will have
    """
⋮----
def add_nav_axes(self, where: Union[Node, str], axes: List[Axis])
⋮----
"""Used to add navigation axes related to the extended array

        Notes
        -----
        For instance the scan axes in the DAQScan
        """
⋮----
"""

        Parameters
        ----------
        where: Union[Node, str]
            the path of a given node or the node itself
        data: DataToExport
        indexes: List[int]
            indexes where to save data in the init h5array (should have the same length as
            extended_shape and with values coherent with this shape
        settings_as_xml: str
            The settings parameter as an XML string
        Keyword Arguments:
            all extra metadata to be saved in the group node where data will be saved

        """
⋮----
class DataLoader
⋮----
"""Specialized Object to load DataWithAxes object from a h5file

    On the contrary to DataSaverLoader, does include navigation axes stored elsewhere in the h5file
    (for instance if saved from the DAQ_Scan)

    Parameters
    ----------
    h5saver: H5SaverLowLevel
    """
⋮----
@property
    def h5saver(self)
⋮----
def walk_nodes(self, where: Union[str, Node] = '/')
⋮----
"""Return a Node generator iterating over the h5file content"""
⋮----
@h5saver.setter
    def h5saver(self, h5saver: H5SaverLowLevel)
⋮----
def get_node(self, where: Union[Node, str], name: str = None) -> Node
⋮----
""" Convenience method to get node"""
⋮----
def get_nav_group(self, where: Union[Node, str]) -> Union[Node, None]
⋮----
"""

        Parameters
        ----------
        where: Union[Node, str]
            the path of a given node or the node itself

        Returns
        -------
        GROUP: returns the group named SPECIAL_GROUP_NAMES['nav_axes'] holding all NavAxis for
        those data

        See Also
        --------
        SPECIAL_GROUP_NAMES
        """
⋮----
while node is not None:  # means we reached the root level
⋮----
node = node.parent_node
⋮----
def load_data(self, where: Union[Node, str], with_bkg=False, load_all=False) -> DataWithAxes
⋮----
"""Load data from a node (or channel node)

        Loaded data contains also nav_axes if any and with optional background subtraction

        Parameters
        ----------
        where: Union[Node, str]
            the path of a given node or the node itself
        with_bkg: bool
            If True will attempt to substract a background data node before loading
        load_all: bool
            If True, will load all data hanging from the same parent node

        Returns
        -------

        """
node_data_type = DataType[self._h5saver.get_node(where).attrs['data_type']]
⋮----
data = self._data_loader.load_data(where, with_bkg=with_bkg, load_all=load_all)
⋮----
nav_group = self.get_nav_group(where)
⋮----
nav_axes = self._axis_loader.get_axes(nav_group)
axes = data.axes[:]
⋮----
def load_all(self, where: GROUP, data: DataToExport, with_bkg=False) -> DataToExport
⋮----
where = self._h5saver.get_node(where)
children_dict = where.children()
data_list = []
⋮----
data_tmp = DataToExport(name=where.name, data=data_list)
</file>

<file path="src/pymodaq_data/h5modules/exporter.py">
# -*- coding: utf-8 -*-
"""
Created the 02/03/2023

@author: Nicolas Tappy
"""
# Standard imports
⋮----
# 3rd party imports
⋮----
# project imports
⋮----
logger = set_logger(get_module_name(__file__))
⋮----
class H5Exporter(metaclass=ABCMeta)
⋮----
"""Base class for an exporter. """
⋮----
# This is to define an abstract class attribute
⋮----
@classmethod
@property
@abstractmethod
    def FORMAT_DESCRIPTION(cls)
⋮----
"""str: file format description as a short text. eg: text file"""
⋮----
@classmethod
@property
@abstractmethod
    def FORMAT_EXTENSION(cls)
⋮----
"""str: File format extension. eg: txt"""
⋮----
def __init__(self)
⋮----
"""Abstract Exporter Constructor"""
⋮----
@abstractmethod
    def export_data(self, node: Node, filename: str) -> None
⋮----
"""Abstract method to save a .h5 node to a file"""
⋮----
class ExporterFactory
⋮----
"""The factory class for creating executors"""
⋮----
exporters_registry = {}
file_filters = {}
⋮----
@classmethod
    def register_exporter(cls) -> Callable
⋮----
"""Class decorator method to register exporter class to the internal registry. Must be used as
        decorator above the definition of an H5Exporter class. H5Exporter must implement specific class
        attributes and methods, see definition: h5node_exporter.H5Exporter
        See h5node_exporter.H5txtExporter and h5node_exporter.H5txtExporter for usage examples.
        returns:
            the exporter class
        """
⋮----
def inner_wrapper(wrapped_class) -> Callable
⋮----
extension = wrapped_class.FORMAT_EXTENSION
format_desc = wrapped_class.FORMAT_DESCRIPTION
⋮----
# Return wrapped_class
⋮----
# Return decorated function
⋮----
@classmethod
    def create_exporter(cls, extension: str, filter: str) -> H5Exporter
⋮----
"""Factory command to create the exporter object.
        This method gets the appropriate executor class from the registry
        and instantiates it.
        Parameters
        ----------
        extension: str
            the extension of the file that will be exported
        filter: str
            the filter string
        Returns
        -------
        an instance of the executor created
        """
⋮----
@classmethod
    def get_file_filters(cls)
⋮----
"""Create the file filters string"""
tmp_list = []
⋮----
@staticmethod
    def get_format_from_filter(filter: str)
⋮----
"""Returns the string format description removing the extension part"""
</file>

<file path="src/pymodaq_data/h5modules/saving.py">
# -*- coding: utf-8 -*-
"""
Created the 15/11/2022

@author: Sebastien Weber
"""
⋮----
config = Config()
logger = set_logger(get_module_name(__file__))
⋮----
class FileType(BaseEnum)
⋮----
detector = 0
actuator = 1
axis = 2
scan = 3
⋮----
class DataType(BaseEnum)
⋮----
data = 'Data'
axis = 'Axis'
live_scan = 'Live'
external_h5 = 'ExtData'
strings = 'Strings'
bkg = 'Bkg'
data_enlargeable = 'EnlData'
error = 'ErrorBar'
⋮----
class H5SaverLowLevel(H5Backend)
⋮----
"""Object containing basic methods in order to structure and interact with a h5file compatible
    with the h5browser

    See Also
    --------
    H5Browser

    Attributes
    ----------
    h5_file: pytables hdf5 file
        object used to save all datas and metadas
    h5_file_path: str or Path
        The file path
    """
⋮----
def __init__(self, save_type: SaveType = 'scan', backend='tables')
⋮----
@property
    def raw_group(self)
⋮----
@property
    def h5_file(self)
⋮----
"""Initializes a new h5 file.

        Parameters
        ----------
        file_name: Path
            a complete Path pointing to a h5 file
        raw_group_name: str
            Base node name
        new_file: bool
            If True create a new file, otherwise append to a potential existing one
        metadata: dict
            A dictionary to be saved as attributes

        Returns
        -------
        bool
            True if new file has been created, False otherwise
        """
datetime_now = datetime.datetime.now()
⋮----
new_file = True
⋮----
self._raw_group.attrs['type'] = self.save_type.name  # first possibility to set a node attribute
self.root().set_attr('file', self.h5_file_name)  # second possibility
⋮----
def save_file(self, filename=None)
⋮----
file_path = Path(filename)
⋮----
def get_set_logger(self, where: Node = None) -> VLARRAY
⋮----
""" Retrieve or create (if absent) a logger enlargeable array to store logs
        Get attributed to the class attribute ``logger_array``
        Parameters
        ----------
        where: node
               location within the tree where to save or retrieve the array

        Returns
        -------
        vlarray
            enlargeable array accepting strings as elements
        """
⋮----
where = self.raw_group
⋮----
where = where.node
logger = 'Logger'
⋮----
# check if logger node exist
⋮----
def add_log(self, msg)
⋮----
def add_string_array(self, where, name, title='', metadata=dict([]))
⋮----
array = self.create_vlarray(where, name, dtype='string', title=title)
⋮----
"""save data arrays on the hdf5 file together with metadata
        Parameters
        ----------
        where: GROUP
            node where to save the array
        name: str
            name of the array in the hdf5 file
        data_type: DataType
            mandatory so that the h5Browser can interpret correctly the array
        data_shape: Iterable
            the shape of the array to save, mandatory if array_to_save is None
        data_dimension: DataDim
         The data's dimension
        scan_shape: Iterable
            the shape of the scan dimensions
        title: str
            the title attribute of the array node
        array_to_save: ndarray or None
            data to be saved in the array. If None, array_type and data_shape should be specified in order to init
            correctly the memory
        array_type: np.dtype or numpy types
            eg np.float, np.int32 ...
        enlargeable: bool
            if False, data are saved as a CARRAY, otherwise as a EARRAY (for ragged data, see add_string_array)
        metadata: dict
            dictionnary whose keys will be saved as the array attributes
        add_scan_dim: if True, the scan axes dimension (scan_shape iterable) is prepended to the array shape on the hdf5
                      In that case, the array is usually initialized as zero and further populated

        Returns
        -------
        array (CARRAY or EARRAY)

        See Also
        --------
        add_data, add_string_array
        """
⋮----
array_type = config('data_saving', 'data_type', 'dynamic')
⋮----
array_type = array_to_save.dtype
⋮----
data_type = enum_checker(DataType, data_type)
data_dimension = enum_checker(DataDim, data_dimension)
⋮----
# if data_shape == (1,):
#     data_shape = None
array = self.create_earray(where, utils.capitalize(name), dtype=np.dtype(array_type),
⋮----
if add_scan_dim:  # means it is an array initialization to zero
shape = list(scan_shape[:])
if not(len(data_shape) == 1 and data_shape[0] == 1):  # means data are not ndarrays of scalars
⋮----
array_to_save = np.zeros(shape, dtype=np.dtype(array_type))
⋮----
array = self.create_carray(where, utils.capitalize(name), obj=array_to_save, title=title)
⋮----
def get_set_group(self, where, name, title='', **kwargs)
⋮----
"""Get the group located at where if it exists otherwise creates it

        This also set the _current_group property
        """
⋮----
def get_groups(self, where: Union[str, GROUP], group_type: Union[str, GroupType, BaseEnum])
⋮----
"""Get all groups hanging from a Group and of a certain type"""
groups = []
⋮----
group_type = group_type.name
⋮----
group = self.get_node(where, node_name)
⋮----
def get_last_group(self, where: GROUP, group_type: Union[str, GroupType, enum.Enum])
⋮----
groups = self.get_groups(where, group_type)
⋮----
def get_node_from_attribute_match(self, where, attr_name, attr_value)
⋮----
"""Get a Node starting from a given node (Group) matching a pair of node attribute name and value"""
⋮----
def get_node_from_title(self, where, title: str)
⋮----
"""Get a Node starting from a given node (Group) matching the given title"""
⋮----
def add_data_group(self, where, data_dim: DataDim, title='', settings_as_xml='', metadata=None)
⋮----
"""Creates a group node at given location in the tree

        Parameters
        ----------
        where: group node
               where to create data group
        group_data_type: DataDim
        title: str, optional
               a title for this node, will be saved as metadata
        settings_as_xml: str, optional
                         XML string created from a Parameter object to be saved as metadata
        metadata: dict, optional
                  will be saved as a new metadata attribute with name: key and value: dict value

        Returns
        -------
        group: group node

        See Also
        --------
        :py:meth:`add_group`
        """
⋮----
metadata = {}
data_dim = enum_checker(DataDim, data_dim)
⋮----
group = self.add_group(data_dim.name, 'data_dim', where, title, metadata)
⋮----
def add_incremental_group(self, group_type: Union[str, GroupType, enum.Enum], where, title='', settings_as_xml='', metadata=None)
⋮----
"""
        Add a node in the h5 file tree of the group type with an increment in the given name
        Parameters
        ----------
        group_type: str or GroupType enum
            one of the possible values of **group_types**
        where: str or node
            parent node where to create the new group
        title: str
            node title
        settings_as_xml: str
            XML string containing Parameter representation
        metadata: dict
            extra metadata to be saved with this new group node

        Returns
        -------
        node: newly created group node
        """
⋮----
nodes = [name for name in self.get_children(self.get_node(where))]
nodes_tmp = []
⋮----
ind_group = -1
⋮----
ind_group = int(nodes_tmp[-1][-3:])
group = self.get_set_group(where, f'{utils.capitalize(group_type.lower())}{ind_group + 1:03d}', title)
⋮----
def add_act_group(self, where, title='', settings_as_xml='', metadata=None)
⋮----
"""
        Add a new group of type detector
        See Also
        -------
        add_incremental_group
        """
⋮----
group = self.add_incremental_group('actuator', where, title, settings_as_xml, metadata)
⋮----
def add_det_group(self, where, title='', settings_as_xml='', metadata=None)
⋮----
group = self.add_incremental_group('detector', where, title, settings_as_xml, metadata)
⋮----
"""Add a new group of type given by the input argument group_type

        At creation adds the attributes description and scan_done to be used elsewhere

        See Also
        -------
        add_incremental_group
        """
⋮----
group = self.add_incremental_group(group_type, where, title, settings_as_xml, metadata)
⋮----
def add_scan_group(self, where='/RawData', title='', settings_as_xml='', metadata=None,)
⋮----
"""Add a new group of type scan

        deprecated, use add_generic_group with a group type as GroupType.scan
        """
⋮----
group = self.add_generic_group(where, title, settings_as_xml, metadata, group_type=GroupType.scan)
⋮----
def add_ch_group(self, where, title='', settings_as_xml='', metadata=None)
⋮----
"""
        Add a new group of type channel
        See Also
        -------
        add_incremental_group
        """
⋮----
group = self.add_incremental_group('ch', where, title, settings_as_xml, metadata)
⋮----
def add_move_group(self, where, title='', settings_as_xml='', metadata=None)
⋮----
"""
        Add a new group of type actuator
        See Also
        -------
        add_incremental_group
        """
</file>

<file path="src/pymodaq_data/h5modules/utils.py">
# -*- coding: utf-8 -*-
"""
Created the 19/01/2023

@author: Sebastien Weber and N Tappy
"""
# Standard imports
⋮----
# 3rd party imports
⋮----
def register_exporter(parent_module_name: str = 'pymodaq_data.h5modules')
⋮----
exporters = []
⋮----
exporter_module = import_module(f'{parent_module_name}.exporters')
⋮----
exporter_path = Path(exporter_module.__path__[0])
⋮----
def register_exporters() -> list
⋮----
exporters = register_exporter('pymodaq_data.h5modules')
discovered_exporter_plugins = get_entrypoints('pymodaq.h5exporters')
⋮----
def find_scan_node(scan_node)
⋮----
"""
    utility function to find the parent node of "scan" type, meaning some of its children (DAQ_scan case)
    or co-nodes (daq_logger case) are navigation axes
    Parameters
    ----------
    scan_node: (pytables node)
        data node from where this function look for its navigation axes if any
    Returns
    -------
    node: the parent node of 'scan' type
    list: the data nodes of type 'navigation_axis' corresponding to the initial data node


    """
⋮----
scan_node = scan_node.parent_node
children = list(scan_node.children().values())  # for data saved using daq_scan
⋮----
scan_node.parent_node.children_name()])  # for data saved using the daq_logger
nav_children = []
⋮----
def get_h5_attributes(self, node_path)
⋮----
"""
        """
node = self.get_node(node_path)
attrs_names = node.attrs.attrs_name
attr_dict = OrderedDict([])
⋮----
# if attr!='settings':
⋮----
settings = None
scan_settings = None
⋮----
settings = node.attrs['settings']
⋮----
scan_settings = node.attrs['scan_settings']
pixmaps = []
⋮----
def get_h5_data_from_node()
⋮----
def extract_axis()
⋮----
def verify_axis_data_uniformity()
</file>

<file path="src/pymodaq_data/plotting/plotter/plotters/matplotlib_plotters.py">
logger = set_logger(get_module_name(__file__))
config = configmod.Config()
⋮----
PLOT_COLORS = utils.plot_colors.copy()
PLOT_COLORS.remove((255, 255, 255))  # remove white color as plotted on white background
PLOT_COLORS = [(np.array(color) / 255).tolist() for color in PLOT_COLORS]  # translation to matplotlib
⋮----
@PlotterFactory.register()
class Plotter(PlotterBase)
⋮----
backend = 'matplotlib'
⋮----
def __init__(self, **_ignored)
⋮----
fig = plt.figure()
⋮----
def plot_dwa(self, dwa: DataWithAxes, *args, **kwargs)
⋮----
def plot_dte(self, dte: DataToExport, *args, **kwargs)
⋮----
def plot1D(self, dwa: DataWithAxes, *args, **kwargs)
⋮----
#plt.title(f'{dwa.name}')
⋮----
def plot2D(self, dwa: DataWithAxes, *args, **kwargs)
⋮----
xaxis = dwa.get_axis_from_index(1)[0]
yaxis = dwa.get_axis_from_index(0)[0]
⋮----
x = xaxis.get_data()
y = yaxis.get_data()
⋮----
# if __name__ == '__main__':
#     from pymodaq.utils import data as data_mod
#     import numpy as np
#     from pymodaq.utils.math_utils import gauss1D, gauss2D
#     from pymodaq.utils.plotting.plotter.plotter import PlotterFactory
#     plotter_factory = PlotterFactory()
#
#     x = np.linspace(0, 100, 101)
#     y = np.linspace(0, 100, 101)
#     y1 = gauss2D(x, 50, 20, y, 40, 7)
⋮----
#     QtWidgets.QApplication.processEvents()
#     dwa = data_mod.DataRaw('mydata', data=[y1, y1, y1],
#                            axes=[data_mod.Axis('xaxis', 'x units', data=x, index=0,
#                                                 spread_order=0),
#                                   data_mod.Axis('yaxis', 'y units', data=y, index=1,
#                                                 spread_order=0)
#                                   ],
#                            labels=['MAG', 'PHASE'],
#                            nav_indexes=())
#     dte = dwa.as_dte('mydte')
#     dwa_mean = dwa.mean()
#     dwa_mean.name = 'mean'
#     dwa_mean_2 = dwa.mean(1)
#     dwa_mean_2.name = 'mean2'
#     dte.append(dwa_mean)
#     dte.append(dwa_mean_2)
⋮----
#     fig = dwa.plot('matplotlib')
#     fig.savefig('myplot.png')
⋮----
#     fig2 = dte.plot('matplotlib')
#     fig2.savefig('mydte.png')
</file>

<file path="src/pymodaq_data/plotting/plotter/plotter.py">
# Standard imports
⋮----
# 3rd party imports
⋮----
# project imports
⋮----
logger = set_logger(get_module_name(__file__))
⋮----
def register_plotter(parent_module_name: str = 'pymodaq_data.plotting.plotter')
⋮----
plotters = []
⋮----
plotter_module = import_module(f'{parent_module_name}.plotters')
⋮----
plotter_path = Path(plotter_module.__path__[0])
⋮----
class PlotterBase(metaclass=ABCMeta)
⋮----
"""Base class for a plotter. """
⋮----
backend: str = abstract_attribute()
def __init__(self)
⋮----
"""Abstract Exporter Constructor"""
⋮----
@abstractmethod
    def plot(self, data, *args, **kwargs) -> None
⋮----
"""Abstract method to plot data object with a given backend"""
⋮----
class PlotterFactory(ObjectFactory)
⋮----
"""Factory class registering and storing interactive plotter"""
⋮----
@classmethod
    def register(cls) -> Callable
⋮----
""" To be used as a decorator

        Register in the class registry a new plotter class using its 2 identifiers: backend and
        data_dim
        """
def inner_wrapper(wrapped_class: PlotterBase) -> Callable
⋮----
key = wrapped_class.backend
⋮----
@classmethod
    def create(cls, key, **kwargs) -> PlotterBase
⋮----
builder = cls._builders[cls.__name__].get(key)
⋮----
def get(self, backend: str, **kwargs)
⋮----
def backends(self) -> List[str]
⋮----
"""Returns the list of plotter backends, main identifier of a given plotter"""
</file>

<file path="src/pymodaq_data/post_treatment/__init__.py">
# -*- coding: utf-8 -*-
"""
Created the 27/10/2022

@author: Sebastien Weber
"""
</file>

<file path="src/pymodaq_data/post_treatment/process_to_scalar.py">
# -*- coding: utf-8 -*-
"""
Created the 04/11/2022

@author: Sebastien Weber
"""
⋮----
config_processors = {
⋮----
class DataProcessorBase(metaclass=ABCMeta)
⋮----
"""Apply processing functions to signal data. This function should return a DataWithAxes.

    Attributes
    ----------
    apply_to: DataDim
        Specify on which type of data dimensionality this processor can be applied to, if only 1D:
        apply_to = DataDim['Data1D']

    """
⋮----
apply_to: DataDim = abstractproperty
⋮----
def process(self, data: DataWithAxes) -> DataWithAxes
⋮----
@abstractmethod
    def operate(self, sub_data: DataWithAxes)
⋮----
@staticmethod
    def flatten_signal_dim(sub_data: DataWithAxes) -> Tuple[Tuple, np.ndarray]
⋮----
"""flattens data's ndarrays along the signal dimensions"""
data_arrays = []
new_shape = [sub_data.shape[ind] for ind in sub_data.nav_indexes]
⋮----
# for each data in subdata, apply the function, here argmax, along the flattened dimension. Then unravel the
# possible multiple indexes (1 for 1D, 2 for 2D)
⋮----
def __call__(self, **kwargs)
⋮----
class DataProcessorFactory(ObjectFactory)
⋮----
def get(self, processor_name, **kwargs) -> DataProcessorBase
⋮----
@property
    def functions(self)
⋮----
"""Get the list of processor functions"""
⋮----
def functions_filtered(self, dim: DataDim)
⋮----
"""Get the list of processor functions that could be applied to data having a given dimensionality"""
⋮----
@DataProcessorFactory.register('mean')
class MeanProcessor(DataProcessorBase)
⋮----
apply_to = DataDim['DataND']
⋮----
def operate(self, sub_data: DataWithAxes)
⋮----
data_arrays = [np.atleast_1d(np.mean(data, axis=sub_data.sig_indexes)) for data in sub_data]
⋮----
@DataProcessorFactory.register('std')
class StdProcessor(DataProcessorBase)
⋮----
data_arrays = [np.atleast_1d(np.std(data, axis=sub_data.sig_indexes)) for data in sub_data]
⋮----
@DataProcessorFactory.register('sum')
class SumProcessor(DataProcessorBase)
⋮----
data_arrays = [np.atleast_1d(np.sum(data, axis=sub_data.sig_indexes)) for data in sub_data]
⋮----
@DataProcessorFactory.register('max')
class MaxProcessor(DataProcessorBase)
⋮----
data_arrays = [np.atleast_1d(np.max(data, axis=sub_data.sig_indexes)) for data in sub_data]
⋮----
@DataProcessorFactory.register('min')
class MinProcessor(DataProcessorBase)
⋮----
data_arrays = [np.atleast_1d(np.min(data, axis=sub_data.sig_indexes)) for data in sub_data]
⋮----
@DataProcessorFactory.register('argmax')
class ArgMaxProcessor(DataProcessorBase)
⋮----
"""Extract info from sub-DataWithAxes

        Retrieve the signal axis values of the maximum position of the data

        Notes
        -----
        For more complex processors, such as the argmin, argmax ... , one cannot use directly the numpy function
        (compared to min, max, mean...). Indeed one has to first flatten the data arrays on the signal axes, then apply
        the function on the flatten dimension, here get the indexes of the minimum along the flattened dimension (as
        a function of the eventual navigations dimensions). From this index, on then obtain as many indexes as signal
        dimensions (1 for 1D Signals, 2 for 2D signals). And we do this for as many data there is in sub_data.
        """
new_data_arrays = []
⋮----
indexes = np.unravel_index(np.nanargmax(dat, len(new_shape)-1), sub_data.shape)[len(sub_data.nav_indexes):]
# from the unraveled index, retrieve the corresponding axis value
⋮----
axis_data = sub_data.get_axis_from_index(sub_data.sig_indexes[ind])[0].get_data()
⋮----
@DataProcessorFactory.register('argmin')
class ArgMinProcessor(DataProcessorBase)
⋮----
"""Extract info from sub-DataWithAxes

        Retrieve the signal axis values of the minimum position of the data

        Notes
        -----
        For more complex processors, such as the argmin, argmax ... , one cannot use directly the numpy function
        (compared to min, max, mean...). Indeed one has to first flatten the data arrays on the signal axes, then apply
        the function on the flatten dimension, here get the indexes of the minimum along the flattened dimension (as
        a function of the eventual navigations dimensions). From this index, on then obtain as many indexes as signal
        dimensions (1 for 1D Signals, 2 for 2D signals). And we do this for as many data there is in sub_data.
        """
⋮----
indexes = np.unravel_index(np.nanargmin(dat, len(new_shape)-1), sub_data.shape)[len(sub_data.nav_indexes):]
⋮----
@DataProcessorFactory.register('argmean')
class ArgMeanProcessor(DataProcessorBase)
⋮----
apply_to = DataDim['Data1D']
⋮----
"""Extract info from sub-DataWithAxes

        Retrieve the signal mean axis values

        Notes
        -----
        For more complex processors, such as the argmin, argmax ... , one cannot use directly the numpy function
        (compared to min, max, mean...). Indeed one has to first flatten the data arrays on the signal axes, then apply
        the function on the flatten dimension, here get the indexes of the minimum along the flattened dimension (as
        a function of the eventual navigations dimensions). From this index, on then obtain as many indexes as signal
        dimensions (1 for 1D Signals, 2 for 2D signals). And we do this for as many data there is in sub_data.
        """
⋮----
values = sub_data.get_axis_from_index(sub_data.sig_indexes[0])[0].get_data()
⋮----
weights = dat
⋮----
@DataProcessorFactory.register('argstd')
class ArgStdProcessor(DataProcessorBase)
⋮----
w_avg = np.atleast_1d(np.average(values, axis=len(new_shape) - 1, weights=weights))
</file>

<file path="src/pymodaq_data/__init__.py">
__version__ = get_version('pymodaq_data')
⋮----
__version__ = '0.0.0dev'
⋮----
logger = set_logger('pymodaq_data', add_handler=True, base_logger=True)
⋮----
ureg = UnitRegistry()
⋮----
Q_ = ureg.Quantity
Unit = ureg.Unit
</file>

<file path="src/pymodaq_data/data.py">
# -*- coding: utf-8 -*-
"""
Created the 28/10/2022

@author: Sebastien Weber
"""
⋮----
config = Config()
plotter_factory = PlotterFactory()
ser_factory = SerializableFactory()
logger = set_logger(get_module_name(__file__))
⋮----
def check_units(units: str)
⋮----
def squeeze(data_array: np.ndarray, do_squeeze=True, squeeze_indexes: Tuple[int]=None) -> np.ndarray
⋮----
""" Squeeze numpy arrays return at least 1D arrays except if do_squeeze is False"""
⋮----
class DataIndexWarning(Warning)
⋮----
class DataTypeWarning(Warning)
⋮----
class DataDimWarning(Warning)
⋮----
class DataSizeWarning(Warning)
⋮----
WARNINGS = [DataIndexWarning, DataTypeWarning, DataDimWarning, DataSizeWarning]
⋮----
class DataShapeError(Exception)
⋮----
class DataLengthError(Exception)
⋮----
class DataDimError(Exception)
⋮----
class DataUnitError(Exception)
⋮----
class DwaType(BaseEnum)
⋮----
"""Different types of `DataWithAxes`."""
DataWithAxes = 0
DataRaw = 1
DataActuator = 2
DataFromPlugins = 3
DataCalculated = 4
⋮----
class DataDim(BaseEnum)
⋮----
"""Enum for dimensionality representation of data"""
Data0D = 0
Data1D = 1
Data2D = 2
DataND = 3
⋮----
def __le__(self, other_dim: 'DataDim')
⋮----
other_dim = enum_checker(DataDim, other_dim)
⋮----
def __lt__(self, other_dim: 'DataDim')
⋮----
def __ge__(self, other_dim: 'DataDim')
⋮----
def __gt__(self, other_dim: 'DataDim')
⋮----
@property
    def dim_index(self)
⋮----
@staticmethod
    def from_data_array(data_array: np.ndarray)
⋮----
class DataSource(BaseEnum)
⋮----
"""Enum for source of data"""
raw = 0
calculated = 1
⋮----
class DataDistribution(BaseEnum)
⋮----
"""Enum for distribution of data"""
uniform = 0
spread = 1
⋮----
def _compute_slices_from_axis(axis: Axis, _slice, *ignored, is_index=True, **ignored_also)
⋮----
_slice = axis.find_index(_slice)
⋮----
start = axis.find_index(
stop = axis.find_index(
_slice = slice(start, stop)
⋮----
@ser_factory.register_decorator()
class Axis(SerializableBase)
⋮----
"""Object holding info and data about physical axis of some data

    In case the axis's data is linear, store the info as a scale and offset else store the data

    Parameters
    ----------
    label: str
        The label of the axis, for instance 'time' for a temporal axis
    units: str
        The units of the data in the object, for instance 's' for seconds
    data: ndarray
        A 1D ndarray holding the data of the axis
    index: int
        an integer representing the index of the Data object this axis is related to
    scaling: float
        The scaling to apply to a linspace version in order to obtain the proper scaling
    offset: float
        The offset to apply to a linspace/scaled version in order to obtain the proper axis
    size: int
        The size of the axis array (to be specified if data is None)
    spread_order: int
        An integer needed in the case where data has a spread DataDistribution. It refers to the index along the data's
        spread_index dimension

    Examples
    --------
    >>> axis = Axis('myaxis', units='seconds', data=np.array([1,2,3,4,5]), index=0)
    """
⋮----
base_type = 'Axis'
⋮----
def __new__(cls, *args, **kwargs)
⋮----
@staticmethod
    def serialize(axis: Axis)
⋮----
""" Convert an Axis object into a bytes message together with the info to convert it back

        Parameters
        ----------
        axis: Axis

        Returns
        -------
        bytes: the total bytes message to serialize the Axis

        Notes
        -----

        The bytes sequence is constructed as:

        * serialize the axis label
        * serialize the axis units
        * serialize the axis array
        * serialize the axis
        * serialize the axis spread_order
        """
⋮----
bytes_string = b''
⋮----
@staticmethod
    def deserialize(bytes_str) -> Tuple[Axis, bytes]
⋮----
"""Convert bytes into an Axis object

        Convert the first bytes into an Axis reading first information about the Axis

        Returns
        -------
        Axis: the decoded Axis
        bytes: the remaining bytes string if any
        """
⋮----
axis = Axis(axis_label, axis_units, data=axis_array, index=axis_index,
⋮----
@staticmethod
    def from_quantity(quantity: Q_[np.ndarray], label='axis', index=0) -> Axis
⋮----
def copy(self)
⋮----
def as_dwa(self, set_itself_as_axis=False) -> DataWithAxes
⋮----
dwa = DataRaw(self.label, units=self.units,
⋮----
@property
    def label(self) -> str
⋮----
"""str: get/set the label of this axis"""
⋮----
@label.setter
    def label(self, lab: str)
⋮----
@property
    def units(self) -> str
⋮----
"""str: get/set the units for this axis without conversion (equivalent to
        force_units)"""
⋮----
@units.setter
    def units(self, units: str)
⋮----
def force_units(self, units: str)
⋮----
""" Change immediately the units to whatever else. Use this with care!"""
units = check_units(units)
⋮----
def units_as(self, units: str, inplace=True, context: str = None, **context_kwargs)
⋮----
#self._units = units
⋮----
def to_reduced_units(self, inplace=False)
⋮----
quantity = self.get_quantity().to_reduced_units()
⋮----
def to_base_units(self, inplace=False)
⋮----
quantity = self.get_quantity().to_base_units()
⋮----
@property
    def index(self) -> int
⋮----
"""int: get/set the index this axis corresponds to in a DataWithAxis object"""
⋮----
@index.setter
    def index(self, ind: int)
⋮----
@property
    def data(self)
⋮----
"""np.ndarray: get/set the data of Axis"""
⋮----
@data.setter
    def data(self, data: Union[np.ndarray, Q_])
⋮----
data = self._check_data_valid(data)
⋮----
def get_data(self) -> np.ndarray
⋮----
"""Convenience method to obtain the axis data (usually None because scaling and offset are used)"""
⋮----
def get_quantity(self) -> Q_
⋮----
""" Convenience method to obtain the numerical data as a quantity array"""
⋮----
def get_data_at(self, indexes: Union[int, IterableType, slice]) -> np.ndarray
⋮----
""" Get data at specified indexes

        Parameters
        ----------
        indexes:
        """
⋮----
indexes = np.array(indexes)
⋮----
def get_scale_offset_from_data(self, data: np.ndarray = None)
⋮----
"""Get the scaling and offset from the axis's data

        If data is not None, extract the scaling and offset

        Parameters
        ----------
        data: ndarray
        """
⋮----
data = self._data
⋮----
def is_axis_linear(self, data=None)
⋮----
data = self.get_data()
⋮----
@property
    def scaling(self)
⋮----
@scaling.setter
    def scaling(self, _scaling: float)
⋮----
@property
    def offset(self)
⋮----
@offset.setter
    def offset(self, _offset: float)
⋮----
@property
    def size(self) -> int
⋮----
"""int: get/set the size/length of the 1D ndarray"""
⋮----
@size.setter
    def size(self, _size: int)
⋮----
@staticmethod
    def _check_index_valid(index: int)
⋮----
def _check_data_valid(self, data: Union[np.ndarray, Q_]) -> np.ndarray
⋮----
data = data.magnitude
⋮----
def _linear_data(self, nsteps: int)
⋮----
"""create axis data with a linear version using scaling and offset"""
⋮----
def create_linear_data(self, nsteps:int)
⋮----
"""replace the axis data with a linear version using scaling and offset"""
⋮----
@staticmethod
    def create_simple_linear_data(nsteps: int)
⋮----
def __len__(self)
⋮----
def _compute_slices(self, _slice, *ignored, is_index=True, **ignored_also)
⋮----
_slice = _compute_slices_from_axis(self, _slice, is_index=is_index)
⋮----
def _slicer(self, _slice, *ignored, is_index=True, **ignored_also)
⋮----
ax: Axis = copy.deepcopy(self)
⋮----
def __getitem__(self, item)
⋮----
# for when axis was a dict
⋮----
def __repr__(self)
⋮----
def __mul__(self, scale: numbers.Real)
⋮----
ax = copy.deepcopy(self)
⋮----
def __add__(self, offset: numbers.Real)
⋮----
def __eq__(self, other: Axis)
⋮----
eq = self.label == other.label
eq = eq and (Unit(self.units).is_compatible_with(other.units))
eq = eq and (self.index == other.index)
⋮----
eq = eq and (np.allclose(Q_(self.data, self.units),
⋮----
eq = eq and (np.allclose(Q_(self.offset, self.units),
eq = eq and (np.allclose(Q_(self.scaling, self.units),
⋮----
def mean(self)
⋮----
def flip(self)
⋮----
""" flip the direction of the axis"""
⋮----
def min(self)
⋮----
def max(self)
⋮----
def find_index(self, threshold: Union[float, Q_]) -> int
⋮----
"""find the index of the threshold value within the axis"""
⋮----
threshold = threshold.m_as(self.units)
⋮----
def find_indexes(self, thresholds: IterableType[Union[float, Q_]]) -> IterableType[int]
⋮----
thresholds = [thresholds]
⋮----
@ser_factory.register_decorator()
class NavAxis(Axis)
⋮----
def __init__(self, *args, **kwargs)
⋮----
class DataLowLevel
⋮----
"""Abstract object for all Data Object

    Parameters
    ----------
    name: str
        the identifier of the data

    Attributes
    ----------
    name: str
    timestamp: float
        Time in seconds since epoch. See method time.time()
    """
⋮----
def __init__(self, name: str)
⋮----
@property
    def name(self)
⋮----
"""Get/Set the identifier of the data"""
⋮----
@name.setter
    def name(self, other_name: str)
⋮----
@property
    def timestamp(self)
⋮----
"""Get/Set the timestamp of when the object has been created"""
⋮----
@timestamp.setter
    def timestamp(self, timestamp: float)
⋮----
"""The timestamp of when the object has been created"""
⋮----
class DataBase(DataLowLevel, NDArrayOperatorsMixin)
⋮----
"""Base object to store homogeneous data and metadata generated by pymodaq's objects.

    To be inherited for real data

    Parameters
    ----------
    name: str
        the identifier of these data
    source: DataSource or str
        Enum specifying if data are raw or processed (for instance from roi)
    dim: DataDim or str
        The identifier of the data type
    distribution: DataDistribution or str
        The distribution type of the data: uniform if distributed on a regular grid or spread if on
        specific unordered points
    data: list of ndarray or Quantities
        The data the object is storing. In case of Quantities, the object units attribute will
        be forced to the unit of this quantity, ignoring the units argument.
    labels: list of str
        The labels of the data nd-arrays
    origin: str
        An identifier of the element where the data originated, for instance the DAQ_Viewer's name.
        Used when appending DataToExport in DAQ_Scan to disintricate from which origin data comes
        from when scanning multiple detectors.
    units: str
        A unit string identifier as specified in the UnitRegistry of the pint module

    kwargs: named parameters
        All other parameters are stored dynamically using the name/value pair. The name of these
        extra parameters are added into the extra_attributes attribute

    Attributes
    ----------
    name: str
        the identifier of these data
    source: DataSource or str
        Enum specifying if data are raw or processed (for instance from roi)
    dim: DataDim or str
        The identifier of the data type
    distribution: DataDistribution or str
        The distribution type of the data: uniform if distributed on a regular grid or spread if on specific
        unordered points
    data: list of ndarray
        The data the object is storing
    labels: list of str
        The labels of the data nd-arrays
    origin: str
        An identifier of the element where the data originated, for instance the DAQ_Viewer's name. Used when appending
        DataToExport in DAQ_Scan to disintricate from which origin data comes from when scanning multiple detectors.
    shape: Tuple[int]
        The shape of the underlying data
    size: int
        The size of the ndarrays stored in the object
    length: int
        The number of ndarrays stored in the object
    extra_attributes: List[str]
        list of string giving identifiers of the attributes added dynamically at the initialization (for instance
        to save extra metadata using the DataSaverLoader

    See Also
    --------
    DataWithAxes, DataFromPlugins, DataRaw, DataSaverLoader

    Examples
    --------
    >>> import numpy as np
    >>> from pymodaq.utils.data import DataBase, DataSource, DataDim, DataDistribution
    >>> data = DataBase('mydata', source=DataSource['raw'], dim=DataDim['Data1D'], \
    distribution=DataDistribution.uniform, data=[np.array([1.,2.,3.]), np.array([4.,5.,6.])],\
    labels=['channel1', 'channel2'], origin='docutils code')
    >>> data.dim
    <DataDim.Data1D: 1>
    >>> data.source
    <DataSource.raw: 0>
    >>> data.shape
    (3,)
    >>> data.length
    2
    >>> data.size
    3
    """
⋮----
base_type = 'Data'
⋮----
# this will make sure, these
# objects are not wrapped by pint
⋮----
source = enum_checker(DataSource, source)
⋮----
distribution = enum_checker(DataDistribution, distribution)
⋮----
self.data = data  # dim consistency is actually checked within the setter method
⋮----
@property
    def units(self)
⋮----
""" Set the object units to the new one (if possible)

        Parameters
        ----------
        units: str
            The new unit to convert the data to
        inplace: bool
            default True.
            If True replace the data's arrays by array in the new units
            If False, return a new data object
        context: str
            See pint documentation
        """
arrays = []
⋮----
new_data = copy.deepcopy(self)
⋮----
def to_base_units(self)
⋮----
dwa = self.deepcopy()
data_quantities = [quantity.to_base_units() for quantity in self.quantities]
⋮----
def value(self, units: str = None) -> float
⋮----
"""Returns the underlying float value (of the first elt in the data list) if this data
        holds only a float otherwise returns a mean of the underlying data

        Parameters
        ----------

        units: str
            if unit is compatible with self.units, convert the data to these new units before
            getting the value


        """
⋮----
data = Q_(float(self.data[0][0]), self.units)
⋮----
data = Q_(float(np.mean(self.data[0])), self.units)
⋮----
def values(self, units: str = None) -> List[float]
⋮----
"""Returns the underlying float value (for each data array in the data list) if this data
        holds only a float otherwise returns a mean of the underlying data"""
⋮----
def as_dte(self, name: str = 'mydte') -> DataToExport
⋮----
"""Convenience method to wrap the DataWithAxes object into a DataToExport"""
⋮----
def add_extra_attribute(self, **kwargs)
⋮----
def get_full_name(self) -> str
⋮----
"""Get the data ful name including the origin attribute into the returned value

        Returns
        -------
        str: the name of the ataWithAxes data constructed as : origin/name

        Examples
        --------
        d0 = DataBase(name='datafromdet0', origin='det0')
        """
⋮----
def __iter__(self)
⋮----
def __next__(self)
⋮----
def __getitem__(self, item) -> np.ndarray
⋮----
def __setitem__(self, key, value)
⋮----
def __array_ufunc__(self, ufunc, method, *inputs, **kwargs)
⋮----
ufunc_name = ufunc.__name__
⋮----
ufunc = HANDLED_UFUNCS[ufunc_name]
⋮----
elts = process_arguments_for_ufuncs(self, inputs)
⋮----
units = dwa.units
ufunc_results = [ufunc(*zipped, **kwargs) for zipped in list(zip(*elts))]
⋮----
ufunc_results = [ufunc_result.to_reduced_units() for ufunc_result in ufunc_results]
units = str(ufunc_results[0].units)
ufunc_results = [ufunc_result.magnitude for ufunc_result in ufunc_results]
⋮----
def __array_function__(self, func, types, args, kwargs)
⋮----
func_name = func.__name__
⋮----
func = HANDLED_FUNCTIONS[func_name]
⋮----
def _comparison_common(self, other, operator='__eq__')
⋮----
if not (# no more checking for name equality but take care ot the pop/remove methods
⋮----
eq = True
⋮----
eq = False
⋮----
eq = eq and np.allclose(self.quantities[ind], other.quantities[ind])
⋮----
eq = eq and np.all(getattr(self.quantities[ind], operator)(other.quantities[ind]))
# extra attributes are not relevant as they may contain module specific data...
# eq = eq and (self.extra_attributes == other.extra_attributes)
# for attribute in self.extra_attributes:
#     eq = eq and (getattr(self, attribute) == getattr(other, attribute))
⋮----
def __bool__(self)
⋮----
def __eq__(self, other)
⋮----
def __ne__(self, other)
⋮----
def __le__(self, other)
⋮----
def __lt__(self, other)
⋮----
def __ge__(self, other)
⋮----
def __gt__(self, other)
⋮----
def deepcopy(self)
⋮----
def average(self, other: 'DataBase', weight: int) -> 'DataBase'
⋮----
""" Compute the weighted average between self and other DataBase

        Parameters
        ----------
        other_data: DataBase
        weight: int
            The weight the 'other' holds with respect to self
        Returns
        -------
        DataBase: the averaged DataBase object
        """
⋮----
def abs(self)
⋮----
""" Take the absolute value of itself"""
⋮----
def angle(self)
⋮----
""" Take the phase value of itself"""
dwa_angle = np.angle(self)
⋮----
def unwrap(self)
⋮----
""" unwrap the underlying array (should be angles otherwise meaningless)"""
⋮----
def real(self)
⋮----
""" Take the real part of itself"""
⋮----
def imag(self)
⋮----
""" Take the imaginary part of itself"""
⋮----
def flipud(self)
⋮----
"""Reverse the order of elements along axis 0 (up/down)"""
new_data = copy.copy(self)
⋮----
def fliplr(self)
⋮----
"""Reverse the order of elements along axis 1 (left/right)"""
⋮----
def append(self, data: DataWithAxes)
⋮----
"""Append data content if the underlying arrays have the same shape and compatible units"""
⋮----
def pop(self, index: int) -> DataBase
⋮----
""" Returns a copy of self but with data taken at the specified index"""
⋮----
@property
    def shape(self)
⋮----
"""The shape of the nd-arrays"""
⋮----
def stack_as_array(self, axis=0, dtype=None) -> np.ndarray
⋮----
""" Stack all data arrays in a single numpy array

        Parameters
        ----------
        axis: int
            The new stack axis index, default 0
        dtype: str or np.dtype
            the dtype of the stacked array

        Returns
        -------
        np.ndarray

        See Also
        --------
        :meth:`np.stack`
        """
⋮----
@property
    def size(self)
⋮----
"""The size of the nd-arrays"""
⋮----
@property
    def dim(self)
⋮----
"""DataDim: the enum representing the dimensionality of the stored data"""
⋮----
def set_dim(self, dim: Union[DataDim, str])
⋮----
"""Addhoc modification of dim independantly of the real data shape,
        should be used with extra care"""
⋮----
@property
    def source(self)
⋮----
"""DataSource: the enum representing the source of the data"""
⋮----
@source.setter
    def source(self, source_type: Union[str, DataSource])
⋮----
source_type = enum_checker(DataSource, source_type)
⋮----
@property
    def distribution(self)
⋮----
"""DataDistribution: the enum representing the distribution of the stored data"""
⋮----
@property
    def length(self)
⋮----
"""The length of data. This is the length of the list containing the nd-arrays"""
⋮----
@property
    def labels(self)
⋮----
@labels.setter
    def labels(self, labels: List['str'])
⋮----
def _check_labels(self, labels: List['str'])
⋮----
labels = []
⋮----
labels = labels[:]
⋮----
def get_data_index(self, index: int = 0) -> np.ndarray
⋮----
"""Get the data by its index in the list, same as self[index]"""
⋮----
def _check_data_type(self, data: List[Union[np.ndarray, Q_]]) -> List[np.ndarray]
⋮----
"""make sure data is a list of nd-arrays"""
is_valid = True
⋮----
is_valid = False
⋮----
# try to transform the data to regular type
⋮----
data = [data.magnitude]
⋮----
data = [data]
⋮----
data = [np.array([data])]
⋮----
data = [array.magnitude for array in data]
⋮----
def check_shape_from_data(self, data: List[np.ndarray])
⋮----
@staticmethod
    def _get_dim_from_data(data: List[np.ndarray]) -> DataDim
⋮----
shape = data[0].shape
size = data[0].size
⋮----
dim = DataDim['Data0D']
⋮----
dim = DataDim['Data1D']
⋮----
dim = DataDim['Data2D']
⋮----
dim = DataDim['DataND']
⋮----
def get_dim_from_data(self, data: List[np.ndarray])
⋮----
"""Get the dimensionality DataDim from data"""
⋮----
def _check_shape_dim_consistency(self, data: List[np.ndarray])
⋮----
"""Process the dim from data or make sure data and DataDim are coherent"""
dim = self.get_dim_from_data(data)
⋮----
def _check_same_shape(self, data: List[np.ndarray])
⋮----
"""Check that all nd-arrays have the same shape"""
⋮----
@property
    def quantities(self) -> list[Q_]
⋮----
""" Get the arrays as pint quantities (with units)"""
⋮----
@property
    def data(self) -> List[np.ndarray]
⋮----
"""List[np.ndarray]: get/set (and check) the data the object is storing"""
⋮----
@data.setter
    def data(self, data: List[Union[np.ndarray, Q_]])
⋮----
data = self._check_data_type(data)
⋮----
def to_dict(self)
⋮----
""" Get the data arrays into dictionary whose keys are the labels"""
data_dict = OrderedDict([])
⋮----
def to_dB(self) -> DataBase
⋮----
""" Get a new data object in decibels

        new in 4.3.0
        """
⋮----
class AxesManagerBase
⋮----
def __init__(self, data_shape: Tuple[int], axes: List[Axis], nav_indexes=None, sig_indexes=None, **kwargs)
⋮----
self._data_shape = data_shape[:]  # initial shape needed for self._check_axis
⋮----
@property
    def axes(self)
⋮----
@axes.setter
    def axes(self, axes: List[Axis])
⋮----
@abstractmethod
    def _check_axis(self, axes)
⋮----
@abstractmethod
    def get_sorted_index(self, axis_index: int = 0, spread_index=0) -> Tuple[np.ndarray, Tuple[slice]]
⋮----
""" Get the index to sort the specified axis

        Parameters
        ----------
        axis_index: int
            The index along which one should sort the data
        spread_index: int
            for spread data only, specifies which spread axis to use

        Returns
        -------
        np.ndarray: the sorted index from the specified axis
        tuple of slice:
            used to slice the underlying data
        """
⋮----
@abstractmethod
    def get_axis_from_index_spread(self, index: int, spread_order: int) -> Axis
⋮----
"""in spread mode, different nav axes have the same index (but not
        the same spread_order integer value)

        """
⋮----
def compute_sig_indexes(self)
⋮----
_shape = list(self._data_shape)
indexes = list(np.arange(len(self._data_shape)))
⋮----
def _has_get_axis_from_index(self, index: int)
⋮----
"""Check if the axis referred by a given data dimensionality index is present

        Returns
        -------
        bool: True if the axis has been found else False
        Axis or None: return the axis instance if has the axis else None
        """
⋮----
def _manage_named_axes(self, axes, x_axis=None, y_axis=None, nav_x_axis=None, nav_y_axis=None)
⋮----
"""This method make sur old style Data is still compatible, especially when using x_axis or y_axis parameters"""
modified = False
⋮----
modified = True
index = 0
⋮----
# in case of Data1D the x_axis corresponds to the first data dim
⋮----
# in case of Data2D the x_axis corresponds to the second data dim (columns)
index = 1
⋮----
# in case of Data2D the y_axis corresponds to the first data dim (lines)
⋮----
# in case of DataND the y_axis corresponds to the first data dim (lines)
⋮----
@property
    def shape(self) -> Tuple[int]
⋮----
# self._data_shape = self.compute_shape_from_axes()
⋮----
@abstractmethod
    def compute_shape_from_axes(self)
⋮----
@property
    def sig_shape(self) -> tuple
⋮----
@property
    def nav_shape(self) -> tuple
⋮----
def append_axis(self, axis: Axis)
⋮----
@property
    def nav_indexes(self) -> IterableType[int]
⋮----
@nav_indexes.setter
    def nav_indexes(self, nav_indexes: IterableType[int])
⋮----
nav_indexes = tuple(nav_indexes)
valid = True
⋮----
valid = False
⋮----
@property
    def sig_indexes(self) -> IterableType[int]
⋮----
@sig_indexes.setter
    def sig_indexes(self, sig_indexes: IterableType[int])
⋮----
sig_indexes = tuple(sig_indexes)
⋮----
@property
    def nav_axes(self) -> List[int]
⋮----
@nav_axes.setter
    def nav_axes(self, nav_indexes: List[int])
⋮----
def is_axis_signal(self, axis: Axis) -> bool
⋮----
"""Check if an axis is considered signal or navigation"""
⋮----
def is_axis_navigation(self, axis: Axis) -> bool
⋮----
"""Check if an axis  is considered signal or navigation"""
⋮----
@abstractmethod
    def get_shape_from_index(self, index: int) -> int
⋮----
"""Get the data shape at the given index"""
⋮----
def get_axes_index(self) -> List[int]
⋮----
"""Get the index list from the axis objects"""
⋮----
@abstractmethod
    def get_axis_from_index(self, index: int, create: bool = False) -> List[Axis]
⋮----
def get_axis_from_index_spread(self, index: int, spread_order: int) -> Axis
⋮----
"""Only valid for Spread data"""
⋮----
def get_nav_axes(self) -> List[Axis]
⋮----
"""Get the navigation axes corresponding to the data

        Use get_axis_from_index for all index in self.nav_indexes, but in spread distribution, one index may
        correspond to multiple nav axes, see Spread data distribution


        """
⋮----
def get_signal_axes(self)
⋮----
axes = []
⋮----
axes_tmp = copy.copy(self.get_axis_from_index(index, create=True))
⋮----
@abstractmethod
    def _get_dimension_str(self)
⋮----
class AxesManagerUniform(AxesManagerBase)
⋮----
def compute_shape_from_axes(self)
⋮----
shape = []
⋮----
shape = self._data_shape
⋮----
def get_shape_from_index(self, index: int) -> int
⋮----
def _check_axis(self, axes: List[Axis])
⋮----
"""Check all axis to make sure of their type and make sure their data are properly referring to the data index

        See Also
        --------
        :py:meth:`Axis.create_linear_data`
        """
⋮----
def get_axis_from_index(self, index: int, create: bool = False) -> List[Axis]
⋮----
"""Get the axis referred by a given data dimensionality index

        If the axis is absent, create a linear one to fit the data shape if parameter create is True

        Parameters
        ----------
        index: int
            The index referring to the data ndarray shape
        create: bool
            If True and the axis referred by index has not been found in axes, create one

        Returns
        -------
        List[Axis] or None: return the list of axis instance if Data has the axis (or it has been created) else None

        See Also
        --------
        :py:meth:`Axis.create_linear_data`
        """
index = int(index)
⋮----
axis = Axis(index=index, offset=0, scaling=1)
⋮----
def get_sorted_index(self, axis_index: int = 0, spread_index=0) -> Tuple[np.ndarray, Tuple[slice]]
⋮----
axes = self.get_axis_from_index(axis_index)
⋮----
sorted_index = np.argsort(axes[0].get_data())
⋮----
slices = []
⋮----
slices = tuple(slices)
⋮----
def _get_dimension_str(self)
⋮----
string = "("
⋮----
string = string.rstrip(", ")
⋮----
class AxesManagerSpread(AxesManagerBase)
⋮----
"""For this particular data category, some explanation is needed, see example below:

    Examples
    --------
    One take images data (20x30) as a function of 2 parameters, say xaxis and yaxis non-linearly spaced on a regular
    grid.

    data.shape = (150, 20, 30)
    data.nav_indexes = (0,)

    The first dimension (150) corresponds to the navigation (there are 150 non uniform data points taken)
    The  second and third could correspond to signal data, here an image of size (20x30)
    so:
    * nav_indexes is (0, )
    * sig_indexes are (1, 2)

    xaxis = Axis(name=xaxis, index=0, data...) length 150
    yaxis = Axis(name=yaxis, index=0, data...) length 150

    In fact from such a data shape the number of navigation axes in unknown . In our example, they are 2. To somehow
    keep track of some ordering in these navigation axes, one adds an attribute to the Axis object: the spread_order 
    xaxis = Axis(name=xaxis, index=0, spread_order=0, data...) length 150
    yaxis = Axis(name=yaxis, index=0, spread_order=1, data...) length 150
    """
⋮----
"""Check all axis to make sure of their type and make sure their data are properly referring to the data index

        """
⋮----
"""Get data shape from axes

        First get the nav length from one of the navigation axes
        Then check for signal axes
        """
⋮----
axes = sorted(self.axes, key=lambda axis: axis.index)
⋮----
"""in spread mode, different nav axes have the same index (but not
        the same spread_order integer value) so may return multiple axis

        No possible "linear" creation in this mode except if the index is a signal index

        """
⋮----
axis = [None]
has_axis = False
⋮----
axis = self.get_axis_from_index_spread(axis_index, spread_index)
⋮----
axis = self.get_axis_from_index(axis_index)[0]
⋮----
sorted_index = np.argsort(axis.get_data())
⋮----
if slices[-1] is Ellipsis:  # only one ellipsis
⋮----
string = f'({self._data_shape})'
⋮----
@ser_factory.register_decorator()
class DataWithAxes(DataBase, SerializableBase)
⋮----
"""Data object with Axis objects corresponding to underlying data nd-arrays

    Parameters
    ----------
    axes: list of Axis
        the list of Axis object for proper plotting, calibration ...
    nav_indexes: tuple of int
        highlight which Axis in axes is Signal or Navigation axis depending on the content:
        For instance, nav_indexes = (2,), means that the axis with index 2 in a at least 3D ndarray data is the first
        navigation axis
        For instance, nav_indexes = (3,2), means that the axis with index 3 in a at least 4D ndarray data is the first
        navigation axis while the axis with index 2 is the second navigation Axis. Axes with index 0 and 1 are signal
        axes of 2D ndarray data
    errors: list of ndarray.
        The list should match the length of the data attribute while the ndarrays
        should match the data ndarray
    """
⋮----
cls.deserialize)  # implement
# serialization/deserialization to all subtypes of DataBase but only when first instantiated hence the decorator
⋮----
nav_indexes = kwargs.pop('nav_axes')
⋮----
x_axis = kwargs.pop('x_axis') if 'x_axis' in kwargs else None
y_axis = kwargs.pop('y_axis') if 'y_axis' in kwargs else None
⋮----
nav_x_axis = kwargs.pop('nav_x_axis') if 'nav_x_axis' in kwargs else None
nav_y_axis = kwargs.pop('nav_y_axis') if 'nav_y_axis' in kwargs else None
⋮----
other_kwargs = dict(x_axis=x_axis, y_axis=y_axis, nav_x_axis=nav_x_axis, nav_y_axis=nav_y_axis)
⋮----
self.get_dim_from_data_axes()  # in DataBase, dim is processed from the shape of data, but if axes are provided
#then use get_dim_from axes
⋮----
@staticmethod
    def serialize(dwa: DataWithAxes)
⋮----
""" Convert a DataWithAxes into a bytes string

        Parameters
        ----------
        dwa: DataWithAxes

        Returns
        -------
        bytes: the total bytes message to serialize the DataWithAxes

        Notes
        -----
        The bytes sequence is constructed as:

        * serialize the timestamp: float
        * serialize the name
        * serialize the source enum as a string
        * serialize the dim enum as a string
        * serialize the distribution enum as a string
        * serialize the list of numpy arrays
        * serialize the list of labels
        * serialize the origin
        * serialize the nav_index tuple as a list of int
        * serialize the list of axis
        * serialize the errors attributes (None or list(np.ndarray))
        * serialize the list of names of extra attributes
        * serialize the extra attributes
        """
⋮----
errors = []  # have to use this extra attribute as if I force dwa.errors = [], it will be
# internally modified as None again
⋮----
errors = dwa.errors
⋮----
@classmethod
    def deserialize(cls, bytes_str: bytes) -> Tuple[DataWithAxes, bytes]
⋮----
"""Convert bytes into a DataWithAxes object

        Convert the first bytes into a DataWithAxes reading first information about the underlying
        data

        Returns
        -------
        DataWithAxes: the decoded DataWithAxes
        bytes: the remaining bytes string if any
        """
⋮----
nav_indexes = tuple(nav_index_list)
⋮----
dwa = cls(name, source=source, dim=dim, distribution=distribution,
⋮----
def check_axes_linear(self, axes: List[Axis] = None) -> bool
⋮----
""" Check if any axis may be non linear

        Should trigger a spread like distribution except id dim is Data1D, in which cas, it doesn't
        matter
        """
⋮----
axes = self._axes
are_axes_linear = True
⋮----
are_axes_linear = are_axes_linear and axis.is_axis_linear()
⋮----
def _check_errors(self, errors: Iterable[np.ndarray])
⋮----
""" Make sure the errors object is adapted to the len/shape of the dwa object

        new in 4.2.0
        """
check = False
⋮----
check = True
⋮----
@property
    def errors(self)
⋮----
""" Get/Set the errors bar values as a list of np.ndarray

        new in 4.2.0
        """
⋮----
@errors.setter
    def errors(self, errors: Iterable[np.ndarray])
⋮----
def get_error(self, index)
⋮----
""" Get a particular error ndarray at the given index in the list

        new in 4.2.0
        """
if self._errors is not None:  #because to the initial check we know it is a list of ndarrays
⋮----
return np.array([0])  # this could be added to any numpy array of any shape
⋮----
def errors_as_dwa(self)
⋮----
""" Get a dwa from self replacing the data content with the error attribute (if not None)

        New in 4.2.0
        """
⋮----
dwa = self.deepcopy_with_new_data(self.errors)
⋮----
""" Call a plotter factory and its plot method over the actual data"""
⋮----
def set_axes_manager(self, data_shape, axes, nav_indexes, **kwargs)
⋮----
is_equal = super().__eq__(other)
⋮----
axes_self = self.get_axis_from_index(ind)
axes_other = other.get_axis_from_index(ind)
⋮----
is_equal = is_equal and other.errors is None
⋮----
def sort_data(self, axis_index: int = 0, spread_index=0, inplace=False) -> DataWithAxes
⋮----
""" Sort data along a given axis, default is 0

        Parameters
        ----------
        axis_index: int
            The index along which one should sort the data
        spread_index: int
            for spread data only, specifies which spread axis to use
        inplace: bool
            modify in place or not the data (and its axes)

        Returns
        -------
        DataWithAxes
        """
⋮----
data = self
⋮----
data = self.deepcopy()
⋮----
def transpose(self)
⋮----
"""replace the data by their transposed version

        Valid only for 2D data
        """
⋮----
def crop_at_along(self, coordinates_tuple: Tuple)
⋮----
axis = self.get_axis_from_index(0)[0]
indexes = axis.find_indexes(coordinates)
⋮----
def mean(self, axis: int = 0) -> DataWithAxes
⋮----
"""Process the mean of the data on the specified axis and returns the new data

        Parameters
        ----------
        axis: int

        Returns
        -------
        DataWithAxes
        """
dat_mean = []
⋮----
mean = np.mean(dat, axis=axis)
⋮----
mean = np.array([mean])
⋮----
def moment(self) -> Tuple[DataWithAxes, DataWithAxes]
⋮----
""" returns the two first moments of the data over the axis

        only valid for Data1D data

        Returns
        -------
        DataCalculated: containing the moment of order 0 (mean)
        DataCalculated: containing the moment of order 1 (std)
        """
⋮----
arrays_mean = []
arrays_std = []
⋮----
def sum(self, axis: int = 0) -> DataWithAxes
⋮----
"""Process the sum of the data on the specified axis and returns the new data

        Parameters
        ----------
        axis: int

        Returns
        -------
        DataWithAxes
        """
dat_sum = []
⋮----
def interp(self,  new_axis_data: Union[Axis, np.ndarray], **kwargs) -> DataWithAxes
⋮----
"""Performs linear interpolation for 1D data only.
        
        For more complex ones, see :py:meth:`scipy.interpolate`

        Parameters
        ----------
        new_axis_data: Union[Axis, np.ndarray]
            The coordinates over which to do the interpolation
        kwargs: dict
            extra named parameters to be passed to the :py:meth:`~numpy.interp` method

        Returns
        -------
        DataWithAxes

        See Also
        --------
        :py:meth:`~numpy.interp`
        :py:meth:`~scipy.interpolate`
        """
⋮----
data_interpolated = []
axis_obj = self.get_axis_from_index(0)[0]
⋮----
new_axis_data = Axis(axis_obj.label, axis_obj.units, data=new_axis_data)
⋮----
new_data = DataCalculated(f'{self.name}_interp', data=data_interpolated,
⋮----
"""Process the Fourier Transform of the data on the specified axis and returns the new data

        Parameters
        ----------
        axis: int
            Apply the FT on this axis index
        axis_label: str
            A new label for the FT computed axis
        axis_units: str
            New units (without conversion on top of the one from the FT) for the computed axis
        labels: List[str]
            list of string for new labels

        Returns
        -------
        DataWithAxes

        See Also
        --------
        :py:meth:`~pymodaq.utils.math_utils.ft`, :py:meth:`~numpy.fft.fft`
        """
dat_ft = []
axis_obj = self.get_axis_from_index(axis)[0].copy()
⋮----
new_data = self.deepcopy_with_new_data(dat_ft)
⋮----
axis_obj = new_data.get_axis_from_index(axis)[0]
⋮----
"""Process the inverse Fourier Transform of the data on the specified axis and returns the
        new data

        Parameters
        ----------
        axis: int
        axis_label: str
            A new label for the FT computed axis
        axis_units: str
            New units (without conversion on top of the one from the FT) for the computed axis
        labels: List[str]
            list of string for new labels

        Returns
        -------
        DataWithAxes

        See Also
        --------
        :py:meth:`~pymodaq.utils.math_utils.ift`, :py:meth:`~numpy.fft.ifft`
        """
dat_ift = []
⋮----
new_data = self.deepcopy_with_new_data(dat_ift)
⋮----
""" Apply 1D curve fitting using the scipy optimization package

        Parameters
        ----------
        function: Callable
            a callable to be used for the fit
        initial_guess: Iterable
            The initial parameters for the fit
        data_index: int
            The index of the data over which to do the fit, if None apply the fit to all
        axis_index: int
            the axis index to use for the fit (if multiple) but there should be only one
        kwargs: dict
            extra named parameters applied to the curve_fit scipy method

        Returns
        -------
        DataCalculated containing the evaluation of the fit on the specified axis

        See Also
        --------
        :py:meth:`~scipy.optimize.curve_fit`
        """
⋮----
axis = self.get_axis_from_index(axis_index)[0].copy()
axis_array = axis.get_data()
⋮----
datalist_to_fit = self.data
labels = [f'{label}_fit' for label in self.labels]
⋮----
datalist_to_fit = [self.data[data_index]]
labels = [f'{self.labels[data_index]}_fit']
⋮----
datalist_fitted = []
fit_coeffs = []
⋮----
def find_peaks(self, height=None, threshold=None, **kwargs) -> DataToExport
⋮----
""" Apply the scipy find_peaks method to 1D data

        Parameters
        ----------
        height: number or ndarray or sequence, optional
        threshold: number or ndarray or sequence, optional
        kwargs: dict
            extra named parameters applied to the find_peaks scipy method

        Returns
        -------
        DataCalculated

        See Also
        --------
        :py:meth:`~scipy.optimize.find_peaks`
        """
⋮----
peaks_indices = []
dte = DataToExport('peaks')
⋮----
def get_dim_from_data_axes(self) -> DataDim
⋮----
"""Get the dimensionality DataDim from data taking into account nav indexes
        """
⋮----
@property
    def n_axes(self)
⋮----
"""Get the number of axes (even if not specified)"""
⋮----
"""convenience property to fetch attribute from axis_manager"""
⋮----
"""convenience property to set attribute from axis_manager"""
⋮----
def axes_limits(self, axes_indexes: List[int] = None) -> List[Tuple[float, float]]
⋮----
"""Get the limits of specified axes (all if axes_indexes is None)"""
⋮----
@property
    def sig_indexes(self)
⋮----
@property
    def nav_indexes(self)
⋮----
@nav_indexes.setter
    def nav_indexes(self, indexes: List[int])
⋮----
"""create new axis manager with new navigation indexes"""
⋮----
def get_sig_index(self) -> List[Axis]
⋮----
def get_nav_axes_with_data(self) -> List[Axis]
⋮----
"""Get the data's navigation axes making sure there is data in the data field"""
axes = self.get_nav_axes()
⋮----
def get_axis_indexes(self) -> List[int]
⋮----
"""Get all present different axis indexes"""
⋮----
def get_axis_from_index(self, index, create=False)
⋮----
def get_axis_from_index_spread(self, index: int, spread: int)
⋮----
def get_axis_from_label(self, label: str) -> Axis
⋮----
"""Get the axis referred by a given label

        Parameters
        ----------
        label: str
            The label of the axis

        Returns
        -------
        Axis or None: return the axis instance if it has the right label else None
        """
⋮----
def create_missing_axes(self)
⋮----
"""Check if given the data shape, some axes are missing to properly define the data
        (especially for plotting)"""
axes = self.axes[:]
⋮----
axes_tmp = self.get_axis_from_index(index, create=True)
⋮----
def _compute_slices(self, slices, is_navigation=True, is_index=True)
⋮----
"""Compute the total slice to apply to the data

        Filling in Ellipsis when no slicing should be done
        Parameters
        ----------
        slices: List of slice
        is_navigation: bool
        is_index: bool
            if False, the slice are on the values of the underlying axes
        Returns
        -------
        list(slice): the computed slices as index (eventually for all axes)
        list(slice): a version as index of the input argument
        """
_slices_as_index = []
⋮----
slices = [slices]
⋮----
indexes = self._am.nav_indexes
⋮----
indexes = self._am.sig_indexes
total_slices = []
slices = list(slices)
⋮----
_slice = slices.pop(0)
⋮----
axis = self.get_axis_from_index(ind)[0]
_slice = _compute_slices_from_axis(axis, _slice, is_index=is_index)
⋮----
total_slices = tuple(total_slices)
⋮----
def check_squeeze(self, total_slices: IterableType[slice], is_navigation: bool)
⋮----
do_squeeze = True
⋮----
do_squeeze = False
⋮----
def _slicer(self, slices, is_navigation=True, is_index=True)
⋮----
"""Apply a given slice to the data either navigation or signal dimension

        Parameters
        ----------
        slices: tuple of slice or int
            the slices to apply to the data
        is_navigation: bool
            if True apply the slices to the navigation dimension else to the signal ones
        is_index: bool
            if True the slices are indexes otherwise the slices are axes values (float or quantities) to be indexed first

        Returns
        -------
        DataWithAxes
            Object of the same type as the initial data, derived from DataWithAxes. But with lower
            data size due to the slicing and with eventually less axes.
        """
⋮----
do_squeeze = self.check_squeeze(total_slices, is_navigation)
new_arrays_data = [squeeze(dat[total_slices], do_squeeze) for dat in self.data]
tmp_axes = self._am.get_signal_axes() if is_navigation else self._am.get_nav_axes()
axes_to_append = [copy.deepcopy(axis) for axis in tmp_axes]
⋮----
# axes_to_append are the axes to append to the new produced data
# (basically the ones to keep)
⋮----
indexes_to_get = self.nav_indexes if is_navigation else self.sig_indexes
# indexes_to_get are the indexes of the axes where the slice should be applied
⋮----
_indexes = list(self.nav_indexes)
⋮----
lower_indexes = dict(zip(_indexes, [0 for _ in range(len(_indexes))]))
# lower_indexes will store for each *axis index* how much the index should be reduced
# because one axis has
# been removed
⋮----
nav_indexes = [] if is_navigation else list(self._am.nav_indexes)
⋮----
ax = self._am.get_axis_from_index(indexes_to_get[ind_slice])
⋮----
if not(ax[0] is None or ax[0].size <= 1):  # means the slice kept part of the axis
⋮----
for axis in axes_to_append:  # means we removed one of the axes (and data dim),
# hence axis index above current index should be lowered by 1
⋮----
distribution = self.distribution
⋮----
distribution = DataDistribution.uniform
⋮----
data = DataWithAxes(self.name, data=new_arrays_data, nav_indexes=tuple(nav_indexes),
⋮----
"""deepcopy without copying the initial data (saving memory)

        The new data, may have some axes stripped as specified in remove_axes_index

        Parameters
        ----------
        data: list of numpy ndarray
            The new data
        remove_axes_index: tuple of int
            indexes of the axis to be removed
        source: DataSource
        keep_dim: bool
            if False (the default) will calculate the new dim based on the data shape
            else keep the same (be aware it could lead to issues)

        Returns
        -------
        DataWithAxes
        """
⋮----
old_data = self.data
⋮----
new_data = self.deepcopy()
⋮----
remove_axes_index = [remove_axes_index]
⋮----
lower_indexes = dict(zip(new_data.get_axis_indexes(),
# lower_indexes will store for each *axis index* how much the index should be reduced because one axis has
# been removed
⋮----
nav_indexes = list(new_data.nav_indexes)
sig_indexes = list(new_data.sig_indexes)
⋮----
# for ind, nav_ind in enumerate(nav_indexes):
#     if nav_ind > index and nav_ind not in remove_axes_index:
#         nav_indexes[ind] -= 1
⋮----
# for ind, sig_ind in enumerate(sig_indexes):
#     if sig_ind > index:
#         sig_indexes[ind] -= 1
⋮----
# new_data._am.sig_indexes = tuple(sig_indexes)
⋮----
@property
    def _am(self) -> AxesManagerBase
⋮----
def get_data_dimension(self) -> str
⋮----
def get_data_as_dwa(self, index: int = 0) -> DataWithAxes
⋮----
""" Get the underlying data selected from the list at index, returned as a DataWithAxes"""
⋮----
@ser_factory.register_decorator()
class DataRaw(DataWithAxes)
⋮----
"""Specialized DataWithAxes set with source as 'raw'. To be used for raw data"""
⋮----
@ser_factory.register_decorator()
class DataCalculated(DataWithAxes)
⋮----
"""Specialized DataWithAxes set with source as 'calculated'. To be used for
    processed/calculated data"""
⋮----
@ser_factory.register_decorator()
class DataFromRoi(DataCalculated)
⋮----
"""Specialized DataWithAxes set with source as 'calculated'.
    To be used for processed data from region of interest"""
⋮----
@ser_factory.register_decorator()
class DataToExport(DataLowLevel, SerializableBase)
⋮----
"""Object to store all raw and calculated DataWithAxes data for later exporting, saving, sending signal...

    Includes methods to retrieve data from dim, source...
    Stored data have a unique identifier their name. If some data is appended with an existing name, it will replace
    the existing data. So if you want to append data that has the same name

    Parameters
    ----------
    name: str
        The identifier of the exporting object
    data: list of DataWithAxes
        All the raw and calculated data to be exported

    Attributes
    ----------
    name
    timestamp
    data
    """
⋮----
# serialization/deserialization to all subtypes of DataToExport
⋮----
def __init__(self, name: str, data: List[DataWithAxes] = [], **kwargs)
⋮----
"""

        Parameters
        ----------
        name
        data
        """
⋮----
@staticmethod
    def serialize(dte: DataToExport)
⋮----
""" Convert a DataToExport into a bytes string

        Parameters
        ----------
        dte: DataToExport

        Returns
        -------
        bytes: the total bytes message to serialize the DataToExport

        Notes
        -----
        The bytes sequence is constructed as:

        * serialize the string type: 'DataToExport'
        * serialize the timestamp: float
        * serialize the name
        * serialize the list of DataWithAxes
        """
⋮----
@classmethod
    def deserialize(cls, bytes_str) -> Tuple[DataToExport, bytes]
⋮----
"""Convert bytes into a DataToExport object

        Convert the first bytes into a DataToExport reading first information about the underlying data

        Returns
        -------
        DataToExport: the decoded DataToExport
        bytes: the remaining bytes if any
        """
⋮----
dte = cls(name, data=data)
⋮----
def plot(self, plotter_backend: str = config('plotting', 'backend'), *args, **kwargs)
⋮----
def affect_name_to_origin_if_none(self)
⋮----
"""Affect self.name to all DataWithAxes children's attribute origin if this origin is not defined"""
⋮----
def __sub__(self, other: object)
⋮----
def __add__(self, other: object)
⋮----
def __mul__(self, other: object)
⋮----
def __truediv__(self, other: object)
⋮----
def average(self, other: DataToExport, weight: int) -> DataToExport
⋮----
""" Compute the weighted average between self and other DataToExport and attributes it to self

        Parameters
        ----------
        other: DataToExport
        weight: int
            The weight the 'other_data' holds with respect to self

        """
⋮----
def merge_as_dwa(self, dim: Union[str, DataDim], name: str = None) -> DataRaw
⋮----
""" attempt to merge filtered dwa into one

        Only possible if all filtered dwa and underlying data have same shape

        Parameters
        ----------
        dim: DataDim or str
            will only try to merge dwa having this dimensionality
        name: str
            The new name of the returned dwa
        """
dim = enum_checker(DataDim, dim)
⋮----
filtered_data = self.get_data_from_dim(dim)
⋮----
dwa = filtered_data[0].deepcopy()
⋮----
name = self.name
⋮----
repr = f'{self.__class__.__name__}: {self.name} <len:{len(self)}>\n'
⋮----
def __next__(self) -> DataWithAxes
⋮----
def __getitem__(self, item) -> Union[DataWithAxes, DataToExport]
⋮----
def __setitem__(self, key, value: DataWithAxes)
⋮----
def get_names(self, dim: DataDim = None) -> List[str]
⋮----
"""Get the names of the stored DataWithAxes,  eventually filtered by dim

        Parameters
        ----------
        dim: DataDim or str

        Returns
        -------
        list of str: the names of the (filtered) DataWithAxes data
        """
⋮----
def get_full_names(self, dim: DataDim = None)
⋮----
"""Get the ful names including the origin attribute into the returned value,  eventually filtered by dim

        Parameters
        ----------
        dim: DataDim or str

        Returns
        -------
        list of str: the names of the (filtered) DataWithAxes data constructed as : origin/name

        Examples
        --------
        d0 = DataWithAxes(name='datafromdet0', origin='det0')
        """
⋮----
def get_origins(self, dim: DataDim = None)
⋮----
"""Get the origins of the underlying data into the returned value,  eventually filtered by dim

        Parameters
        ----------
        dim: DataDim or str

        Returns
        -------
        list of str: the origins of the (filtered) DataWithAxes data

        Examples
        --------
        d0 = DataWithAxes(name='datafromdet0', origin='det0')
        """
⋮----
def get_data_from_full_name(self, full_name: str, deepcopy=False) -> DataWithAxes
⋮----
"""Get the DataWithAxes with matching full name"""
⋮----
data = self.get_data_from_name_origin(full_name.split('/')[1], full_name.split('/')[0]).deepcopy()
⋮----
data = self.get_data_from_name_origin(full_name.split('/')[1], full_name.split('/')[0])
⋮----
def get_data_from_full_names(self, full_names: List[str], deepcopy=False) -> DataToExport
⋮----
data = [self.get_data_from_full_name(full_name, deepcopy) for full_name in full_names]
⋮----
def get_dim_presents(self) -> List[str]
⋮----
dims = []
⋮----
def get_data_from_source(self, source: DataSource, deepcopy=False, sort_by_name=False) -> DataToExport
⋮----
"""Get the data matching the given DataSource

        Returns
        -------
        DataToExport: filtered with data matching the dimensionality
        """
⋮----
def get_data_from_missing_attribute(self, attribute: str, deepcopy=False) -> DataToExport
⋮----
""" Get the data matching a given attribute value

        Parameters
        ----------
        attribute: str
            a string of a possible attribute
        deepcopy: bool
            if True the returned DataToExport will contain deepcopies of the DataWithAxes
        Returns
        -------
        DataToExport: filtered with data missing the given attribute
        """
⋮----
"""Get the data matching a given attribute value

        Parameters
        ----------
        attribute: str
            The name of the attribute to sort data with
        attribute_value: Any
            The value of the attribute
        deepcopy: bool
            If True, the returned data are deepcopied from the original
        sort_by_name: bool
            If True the returned data are sorted alphabetically using their name, default is False

        Returns
        -------
        DataToExport: filtered with data matching the attribute presence and value
        """
selection = find_objects_in_list_from_attr_name_val(self.data, attribute, attribute_value,
⋮----
data = [sel[0].deepcopy() for sel in selection]
⋮----
data = [sel[0] for sel in selection]
⋮----
def get_data_from_dim(self, dim: DataDim, deepcopy=False, sort_by_name=False) -> DataToExport
⋮----
"""Get the data matching the given DataDim

        Returns
        -------
        DataToExport: filtered with data matching the dimensionality
        """
⋮----
def get_data_from_dims(self, dims: List[DataDim], deepcopy=False, sort_by_name=False) -> DataToExport
⋮----
data = DataToExport(name=self.name)
⋮----
def get_data_from_sig_axes(self, Naxes: int, deepcopy: bool = False) -> DataToExport
⋮----
"""Get the data matching the given number of signal axes

        Parameters
        ----------
        Naxes: int
            Number of signal axes in the DataWithAxes objects

        Returns
        -------
        DataToExport: filtered with data matching the number of signal axes
        """
⋮----
def get_data_from_Naxes(self, Naxes: int, deepcopy: bool = False) -> DataToExport
⋮----
"""Get the data matching the given number of axes

        Parameters
        ----------
        Naxes: int
            Number of axes in the DataWithAxes objects

        Returns
        -------
        DataToExport: filtered with data matching the number of axes
        """
⋮----
def get_data_with_naxes_lower_than(self, n_axes=2, deepcopy: bool = False) -> DataToExport
⋮----
"""Get the data with n axes lower than the given number

        Parameters
        ----------
        Naxes: int
            Number of axes in the DataWithAxes objects

        Returns
        -------
        DataToExport: filtered with data matching the number of axes
        """
⋮----
def get_data_from_name(self, name: str) -> DataWithAxes
⋮----
"""Get the data matching the given name"""
⋮----
def get_data_from_names(self, names: List[str]) -> DataToExport
⋮----
def get_data_from_name_origin(self, name: str, origin: str = '') -> DataWithAxes
⋮----
"""Get the data matching the given name and the given origin"""
⋮----
selection = find_objects_in_list_from_attr_name_val(self.data, 'name', name, return_first=False)
selection = [sel[0] for sel in selection]
⋮----
def index(self, data: DataWithAxes)
⋮----
""" Here use a comparison to assert data is equal to one element in the list

        But the __eq__ method is not checking the name while it is the main issue for elt finding
        Hence here I'm doing both checks
        """
⋮----
def index_from_name_origin(self, name: str, origin: str = '') -> List[DataWithAxes]
⋮----
"""Get the index of a given DataWithAxes within the list of data"""
⋮----
data_selection = [sel[0] for sel in selection]
index_selection = [sel[1] for sel in selection]
⋮----
index = index_selection[index]
⋮----
def pop(self, index: int) -> DataWithAxes
⋮----
"""return and remove the DataWithAxes referred by its index

        Parameters
        ----------
        index: int
            index as returned by self.index_from_name_origin

        See Also
        --------
        index_from_name_origin
        """
⋮----
def remove(self, dwa: DataWithAxes)
⋮----
""" Use the DataWithAxes object comparison __eq__ to retrieve the elt to remove

        Parameters
        ----------
        dwa: DataWithAxes
            The da to remove from the list
        """
⋮----
@property
    def data(self) -> List[DataWithAxes]
⋮----
"""List[DataWithAxes]: get the data contained in the object"""
⋮----
@data.setter
    def data(self, new_data: List[DataWithAxes])
⋮----
self._data[:] = [dat for dat in new_data]  # shallow copyto make sure that if the original
# list is changed, the change will not be applied in here
⋮----
@staticmethod
    def _check_data_type(data: DataWithAxes)
⋮----
"""Make sure data is a DataWithAxes object or inherited"""
⋮----
@dispatch(list)
    def append(self, data_list: List[DataWithAxes])
⋮----
@dispatch(DataWithAxes)
    def append(self, dwa: DataWithAxes)
⋮----
"""Append/replace DataWithAxes object to the data attribute

        Make sure only one DataWithAxes object with a given name is in the list except if they don't
        have the same
        origin identifier
        """
dwa = dwa.deepcopy()
⋮----
obj = self.get_data_from_name_origin(dwa.name, dwa.origin)
⋮----
@dispatch(object)
    def append(self, dte: DataToExport)
⋮----
d = DataRaw('hjk', units='m', data=[np.array([0, 1, 2])])
dm = DataRaw('hjk', units='mm', data=[np.array([0, 1, 2])])
⋮----
d1 = DataFromRoi(name=f'Hlineout_', data=[np.zeros((24,))],
d2 = DataFromRoi(name=f'Hlineout_', data=[np.zeros((12,))],
⋮----
Nsig = 200
Nnav = 10
x = np.linspace(-Nsig/2, Nsig/2-1, Nsig)
⋮----
dat = np.zeros((Nnav, Nsig))
⋮----
data = DataRaw('mydata', data=[dat], nav_indexes=(0,),
⋮----
data2 = copy.copy(data)
⋮----
data3 = data.deepcopy_with_new_data([np.sum(dat, 1)], remove_axes_index=(1,))
</file>

<file path="src/pymodaq_data/numpy_func.py">
from pint.facets.numpy.numpy_func import HANDLED_UFUNCS  # imported by the data module
⋮----
HANDLED_FUNCTIONS = {}
⋮----
"""

    Parameters
    ----------
    input: 'DataBase'
    inputs: list of elts in a numpy operation, could be numbers, quantities, ndarray, or 'DataBase'

    Returns
    -------
    list of numbers, quantities or numpy arrays for applying to pint handled functions
    """
elts = []
⋮----
elif isinstance(elt, Q_):  # take its magnitude
⋮----
def implements(np_function)
⋮----
"""Register an __array_function__ implementation for DataWithAxes."""
def decorator(func)
⋮----
# ********* FUNCTIONS that reduce dimensions *****************
⋮----
all_axes = list(dwa.nav_indexes) + list(dwa.sig_indexes)
⋮----
remove_axis = all_axes
⋮----
remove_axis = all_axes[axis]
⋮----
remove_axis = [all_axes[axis_index] for axis_index in axis]
dwa_func = dwa.deepcopy_with_new_data(
⋮----
@implements('max')
def _max(dwa: 'DataWithAxes', *args, axis: Optional[Union[int, Iterable[int]]] = None, **kwargs)
⋮----
@implements('min')
def _min(dwa: 'DataWithAxes', *args, axis: Optional[Union[int, Iterable[int]]] = None, **kwargs)
⋮----
@implements('argmax')
def _argmax(dwa: 'DataWithAxes', *args, axis: Optional[Union[int, Iterable[int]]] = None, **kwargs)
⋮----
@implements('argmin')
def _argmin(dwa: 'DataWithAxes', *args, axis: Optional[Union[int, Iterable[int]]] = None, **kwargs)
⋮----
@implements("std")
def _std(dwa: 'DataWithAxes', *args, axis: Optional[Union[int, Iterable[int]]] = None, **kwargs)
⋮----
@implements("mean")
def _mean(dwa: 'DataWithAxes', *args, axis: Optional[Union[int, Iterable[int]]] = None, **kwargs)
⋮----
@implements("sum")
def _sum(dwa: 'DataWithAxes', *args, axis: Optional[Union[int, Iterable[int]]] = None, **kwargs)
⋮----
# ************* FUNCTIONS that apply with units ********
⋮----
@implements('angle')
def _angle(dwa: 'DataWithAxes', *args, **kwargs)
⋮----
@implements('unwrap')
def _unwrap(dwa: 'DataWithAxes', *args, **kwargs)
⋮----
@implements('real')
def _real(dwa: 'DataWithAxes', *args, **kwargs)
⋮----
@implements('imag')
def _imag(dwa: 'DataWithAxes', *args, **kwargs)
⋮----
@implements('absolute')
def _absolute(dwa: 'DataWithAxes', *args, **kwargs)
⋮----
@implements('abs')
def _abs(dwa: 'DataWithAxes', *args, **kwargs)
⋮----
@implements('roll')
def _roll(dwa: 'DataWithAxes', *args, **kwargs)
⋮----
dwa_func = dwa.deepcopy_with_new_data(data=[np.roll(array, *args, **kwargs) for array in dwa])
⋮----
# ******** functions that return booleans ***********
⋮----
@implements('all')
def _all(dwa: 'DataWithAxes', *args, axis: Optional[Union[int, Iterable[int]]] = None, **kwargs)
⋮----
@implements('any')
def _any(dwa: 'DataWithAxes', *args, axis: Optional[Union[int, Iterable[int]]] = None, **kwargs)
⋮----
dwa = data_mod.DataCalculated(
⋮----
# *************** other numpy function ****************
⋮----
@implements('flipud')
def _flipud(dwa: 'DataWithAxes', *args, **kwargs)
⋮----
dwa_func = dwa.deepcopy_with_new_data([np.flipud(data_array) for data_array in dwa])
⋮----
@implements('fliplr')
def _fliplr(dwa: 'DataWithAxes', *args, **kwargs)
⋮----
dwa_func = dwa.deepcopy_with_new_data([np.fliplr(data_array) for data_array in dwa])
⋮----
@implements('transpose')
def _transpose(dwa: 'DataWithAxes', *args, **kwargs)
⋮----
dwa_func = dwa.deepcopy_with_new_data([np.transpose(data_array) for data_array in dwa])
</file>

<file path="src/pymodaq_data/slicing.py">
# -*- coding: utf-8 -*-
"""
Created the 07/11/2022

@author: Sebastien Weber
"""
⋮----
class SpecialSlicers(object)
⋮----
"""make it elegant to apply a slice to navigation or signal dimensions"""
def __init__(self, obj: Union['DataWithAxes', 'Axis'], is_navigation, is_index=True)
⋮----
def __getitem__(self, slices) -> Union['DataWithAxes', 'Axis']
⋮----
class SpecialSlicersData(SpecialSlicers)
⋮----
def __setitem__(self, slices, data: Union[np.ndarray, 'DataWithAxes', 'Axis'])
⋮----
"""x.__setitem__(slices, data) <==> x[slices]=data
        """
⋮----
data_to_replace = data
⋮----
data_to_replace = data.get_data()
⋮----
else:  # means it's a DataWithAxes
data_to_replace = data[ind]
⋮----
def __len__(self)
</file>

<file path="tests/h5module_test/backend_test.py">
tested_backend = []
⋮----
@pytest.fixture(scope="module")
def session_path(tmp_path_factory)
⋮----
def generate_random_data(shape, dtype=float)
⋮----
@pytest.fixture(params=tested_backend)
def get_backend(request, tmp_path)
⋮----
bck = backends.H5Backend(request.param)
title = 'this is a test file'
start_path = get_temp_path(tmp_path, request.param)
⋮----
def get_temp_path(tmp_path, backend='h5pyd')
⋮----
def test_check_mandatory_attrs()
⋮----
attr_name = 'TITLE'
attr = b'test'
result = backends.check_mandatory_attrs(attr_name, attr)
⋮----
attr = f'test'
⋮----
attr_name = 'TEST'
⋮----
class TestNode
⋮----
@pytest.mark.parametrize('backend', tested_backend)
    def test_init(self, backend)
⋮----
node_dict = {'NAME': 'Node', 'TITLE': 'test'}
node_obj = backends.Node(node_dict, backend)
⋮----
node_obj_2 = backends.Node(node_obj, backend)
⋮----
def test_h5file(self, get_backend)
⋮----
bck = get_backend
attrs = dict(attr1='one attr', attr2=(10, 15), attr3=12.4)
node = bck.add_group('mygroup', 'detector', '/', metadata=attrs)
⋮----
def test_to_backend(self, get_backend)
⋮----
bck_bis = node.to_h5_backend()
⋮----
group_node = bck_bis.get_node('/Mygroup')
⋮----
class TestH5Backend
⋮----
@pytest.mark.parametrize('backend', tested_backend)
    def test_file_open_close(self, tmp_path, backend)
⋮----
bck = backends.H5Backend(backend)
⋮----
start_path = get_temp_path(tmp_path, backend)
h5_file = bck.open_file(start_path.joinpath('h5file.h5'), 'w', title)
⋮----
def test_attrs(self, get_backend)
⋮----
attr_back = bck.root().attrs[attr]
⋮----
attrs_back = bck.root().attrs.attrs_name
⋮----
@pytest.mark.parametrize('group_type', backends.GroupType.names())
    def test_add_group(self, get_backend, group_type)
⋮----
title = 'this is a group'
g3 = bck.add_group('g3', group_type, bck.root(), title=title, metadata=dict(attr1='attr1', attr2=21.4))
⋮----
# this below is not enforced anymore pymodaq_data>5.0.20
# gtype = 'this is not a valid group type'
# with pytest.raises(ValueError):
#     g4 = bck.add_group('g4', gtype, bck.root())
⋮----
def test_group_creation(self, get_backend)
⋮----
g1 = bck.get_set_group(bck.root(), 'g1', title)
g2 = bck.get_set_group('/', 'g2')
⋮----
g21 = bck.get_set_group(g2, 'g21')
⋮----
g22 = bck.get_set_group('/g2', 'g22', title='group g22')
⋮----
# test node methods
⋮----
def test_walk_groups(self, get_backend)
⋮----
g1 = bck.get_set_group(bck.root(), 'g1')
⋮----
groups = ['/', 'g1', 'g2', 'g22', 'g21']
gps = []
⋮----
def test_walk_nodes(self, get_backend)
⋮----
array_g1 = bck.create_carray(g1, 'array_g1', np.array([1, 2, 3, 4, 5, 6]))
⋮----
array_g2 = bck.create_carray(g2, 'array_g2', np.array([1, 2, 3, 4, 5, 6]))
⋮----
array_g22 = bck.create_carray(g22, 'array_g22', np.array([1, 2, 3, 4, 5, 6]))
nodes = ['/', 'g1', 'g2', 'array_g2', 'g22', 'g21', 'array_g1', 'array_g22']
⋮----
def test_carray(self, get_backend)
⋮----
carray1_data = np.random.rand(5, 10)
title = 'this is a carray'
⋮----
carray1 = bck.create_carray(g1, 'carray1', obj=carray1_data, title=title)
⋮----
@pytest.mark.parametrize('compression', ['gzip', 'zlib'])
@pytest.mark.parametrize('comp_level', list(range(0, 10, 3)))
    def test_carray_comp(self, get_backend, compression, comp_level)
⋮----
array_data = np.random.rand(5, 10)
# gzip and zlib filters are compatible, but zlib is used by pytables while gzip is used by h5py
⋮----
array1 = bck.create_carray(g1, 'carray1', obj=array_data)
⋮----
def test_earray(self, get_backend)
⋮----
title = 'this is a earray'
dtype = np.uint32
array = bck.create_earray(g1, 'array', dtype=dtype, title=title)
⋮----
array_shape = (10, 3)
array1 = bck.create_earray(g1, 'array1', dtype=dtype, data_shape=array_shape, title=title)
⋮----
expected_shape = [1]
⋮----
@pytest.mark.parametrize('compression', ['gzip', 'zlib'])
@pytest.mark.parametrize('comp_level', list(range(0, 10, 3)))
    def test_earray_comp(self, get_backend, compression, comp_level)
⋮----
array = bck.create_earray(g1, 'array', dtype=dtype, data_shape=array_shape)
data = generate_random_data(array_shape, dtype)
⋮----
def test_vlarray(self, get_backend)
⋮----
title = 'this is a vlarray'
⋮----
array = bck.create_vlarray(g1, 'array', dtype=dtype, title=title)
⋮----
def test_vlarray_string(self, get_backend)
⋮----
dtype = 'string'
sarray = bck.create_vlarray(g1, 'array', dtype=dtype, title=title)
⋮----
st = 'this is a string'
st2 = 'this is a second string'
⋮----
class TestGroup
⋮----
def test_children(self, get_backend)
⋮----
title = lambda x: f'this is a {x} group'
group = bck.add_group('base group', 'scan', bck.root(), title='base group')
⋮----
n1 = bck.create_carray(group, name='n1', obj=np.zeros((5, 2)), title=title('n1'))
n2 = bck.add_group('n2', 'scan', group, title=title('n2'))
⋮----
assert group.children_name() == ['N2', 'n1']  # notice the capital for the groups...
n3 = bck.add_group('m1', 'scan', group, title=title('n2'))
assert group.children_name() == ['M1', 'N2', 'n1']  # notice the capital for the groups... and the sorted
⋮----
def test_remove_children(self, get_backend)
⋮----
assert group.children_name() == []  # notice the capital for the groups...
</file>

<file path="tests/h5module_test/data_saving_test.py">
# -*- coding: utf-8 -*-
"""
Created the 21/11/2022

@author: Sebastien Weber
"""
⋮----
@pytest.fixture()
def get_h5saver(tmp_path)
⋮----
h5saver = saving.H5SaverLowLevel()
addhoc_file_path = tmp_path.joinpath('h5file.h5')
⋮----
LABEL = 'A Label'
UNITS = 'um'
OFFSET = -20.4
SCALING = 0.22
SIZE = 20
⋮----
DATA = OFFSET + SCALING * np.linspace(0, SIZE-1, SIZE)
⋮----
DATA0D: np.ndarray = np.array([2.7])
DATA1D: np.ndarray = np.arange(0, 10)
DATA2D: np.ndarray = np.arange(0, 5*6).reshape((5, 6))
DATAND: np.ndarray = np.arange(0, 5 * 6 * 3).reshape((5, 6, 3))
⋮----
def create_axis_array(size)
⋮----
def init_axis(data=None, index=0)
⋮----
data = DATA
⋮----
def init_data(data=None, Ndata=1, axes=(), name='myData') -> DataWithAxes
⋮----
data = DATA2D
⋮----
@pytest.fixture()
def init_data_to_export()
⋮----
Ndata = 2
⋮----
data2D = DataWithAxes(name='mydata2D', data=[DATA2D for _ in range(Ndata)],
⋮----
data1D = DataWithAxes(name='mydata1D', data=[DATA1D for _ in range(Ndata)],
⋮----
data0D = DataWithAxes(name='mydata0D', data=[DATA0D for _ in range(Ndata)],
⋮----
data0Dbis = DataWithAxes(name='mydata0Dbis', data=[DATA0D for _ in range(Ndata)],
⋮----
data_to_export = DataToExport(name='mybigdata', data=[data2D, data0D, data1D, data0Dbis])
⋮----
class TestAxisSaverLoader
⋮----
def test_init(self, get_h5saver)
⋮----
h5saver = get_h5saver
axis_saver = AxisSaverLoader(h5saver)
⋮----
def test_add_axis(self, get_h5saver)
⋮----
SIZE = 10
OFFSET = -5.
SCALING = 0.2
INDEX = 5
LABEL = 'myaxis'
UNITS = 'ms'
axis = Axis(label=LABEL, units=UNITS,
⋮----
axis_node = axis_saver.add_axis(h5saver.raw_group, axis)
⋮----
attrs = ['label', 'units', 'offset', 'scaling', 'index']
attrs_values = [LABEL, UNITS, OFFSET, SCALING, INDEX]
⋮----
def test_load_axis(self, get_h5saver)
⋮----
axis_back = axis_saver.load_axis(axis_node)
⋮----
def test_add_multiple_axis(self, get_h5saver)
⋮----
UNITS = 'myunits'
axes_ini = []
⋮----
axis_node = axis_saver.add_axis(h5saver.raw_group, axes_ini[ind])
⋮----
axes_out = axis_saver.get_axes(h5saver.raw_group)
⋮----
class TestDataSaverLoader
⋮----
data_saver = DataSaverLoader(h5saver)
⋮----
def test_add_data(self, get_h5saver)
⋮----
data = DataWithAxes(name='mydata', data=[DATA2D for _ in range(Ndata)], labels=['mylabel1', 'mylabel2'],
⋮----
def test_add_data_with_errors(self, get_h5saver)
⋮----
errors = [np.random.random_sample(DATA2D.shape) for _ in range(Ndata)]
⋮----
data = DataWithAxes(name='mydata', data=[DATA2D for _ in range(Ndata)],
⋮----
def test_load_data(self, get_h5saver)
⋮----
errors = [np.random.random_sample(DATA1D.shape) for _ in range(Ndata)]
⋮----
data = DataWithAxes(name='mydata', data=[DATA1D for _ in range(Ndata)],
⋮----
loaded_data = data_saver.load_data(h5saver.get_node('/RawData/Data00'), load_all=True)
⋮----
loaded_data = data_saver.load_data(h5saver.get_node('/RawData/Data01'), load_all=True)
⋮----
loaded_data = data_saver.load_data(h5saver.get_node(f'/RawData/Data0{INDEX}'), load_all=False)
⋮----
def test_load_with_bkg(self, get_h5saver)
⋮----
bkgSaver = BkgSaver(h5saver)
⋮----
axes = [Axis(data=create_axis_array(DATA2D.shape[0]), label='myaxis0', units='mm',
⋮----
loaded_data = data_saver.load_data(h5saver.get_node('/RawData/Data01'), load_all=True, with_bkg=True)
⋮----
def test_extra_attributes_and_timestamping(self, get_h5saver)
⋮----
node = h5saver.get_node('/RawData/Data01')
⋮----
class TestBkgSaver
⋮----
axes = [Axis(data=create_axis_array(DATA2D.shape[0]), label='myaxis0', units='ms',
⋮----
data_bkg = init_data(DATA2D, axes=axes, name='mykbg')
⋮----
data_bkg_loaded = bkgSaver.load_data('/RawData/Bkg00')
⋮----
class TestDataEnlargeableSaver
⋮----
data_saver = DataEnlargeableSaver(h5saver)
⋮----
@pytest.mark.parametrize('Nenl', [1, 2, 3])
@pytest.mark.parametrize('data_array', [DATA0D, DATA1D, DATA2D])
    def test_add_data(self, get_h5saver, data_array, Nenl)
⋮----
axis_values = tuple(np.random.randn(Nenl))
data_saver = DataEnlargeableSaver(h5saver,
⋮----
data = DataWithAxes(name='mydata', data=[data_array for _ in range(Ndata)],
⋮----
data_node = h5saver.get_node('/RawData/EnlData00')
⋮----
ESHAPE = [1]
⋮----
ESHAPE = [2]
⋮----
dwa_back = data_saver.load_data('/RawData/EnlData00')
⋮----
def test_add_data_ndviewer_0D(self, get_h5saver)
⋮----
Npts = 11
Ndata = 1
data_array_0D = np.linspace(0, 100, Npts)
axis_array = np.linspace(0, 1, Npts)
axis_values = Axis('enlaxis', units='s', data=axis_array)
Nenl=1
⋮----
data = DataRaw(name='mynddata', data=[data_array_0D for _ in range(Ndata)],
⋮----
def test_add_data_ndviewer_1D(self, get_h5saver)
⋮----
Npts_nav = 11
Npts_sig = 21
⋮----
data_array_1D_1D = np.arange(Npts_sig*Npts_nav).reshape((Npts_nav, Npts_sig))
axis_array_sig = np.linspace(0, 100, Npts_sig)
axis_array_nav = np.linspace(0, 1, Npts_nav)
axis_nav = Axis('enlaxis', units='s', data=axis_array_nav, index=0)
axis_sig = Axis('sigaxis', units='m', data=axis_array_sig, index=1)
⋮----
data = DataRaw(name='mynddata', data=[data_array_1D_1D for _ in range(Ndata)],
⋮----
class TestDataExtendedSaver
⋮----
EXT_SHAPE = (5, 10)
data_saver = DataExtendedSaver(h5saver, EXT_SHAPE)
⋮----
data_ext_shape = list(EXT_SHAPE)
⋮----
INDEXES = [4, 3]
⋮----
data_node = h5saver.get_node(f'/RawData/Data0{ind}')
⋮----
class TestDataToExportSaver
⋮----
def test_save(self, get_h5saver, init_data_to_export)
⋮----
data_to_export = init_data_to_export
⋮----
data_saver = DataToExportSaver(h5saver)
⋮----
det_group = h5saver.get_set_group(h5saver.raw_group, 'MyDet')
⋮----
class TestDataToExportEnlargeableSaver
⋮----
data_saver = DataToExportEnlargeableSaver(h5saver)
⋮----
Nadd_data = 2
⋮----
@pytest.mark.parametrize('data_array', [DATA0D, DATA1D, DATA2D])
@pytest.mark.parametrize('Nenl', [1, 2, 3])
    def test_spread_data(self, get_h5saver, Nenl, data_array)
⋮----
dte_saver = DataToExportEnlargeableSaver(h5saver,
dte_loader = DataLoader(h5saver)
⋮----
axis_values = list(np.random.randn(Nenl))
⋮----
dwa = DataRaw('dwa', data=[data_array],
⋮----
dte = DataToExport('dte', data=[dwa])
⋮----
data_loaded = dte_loader.load_data(
⋮----
class TestDataToExportTimedSaver
⋮----
data_saver = DataToExportTimedSaver(h5saver)
⋮----
class TestDataToExportExtendedSaver
⋮----
nav_axes = []
⋮----
data_saver = DataToExportExtendedSaver(h5saver, extended_shape=EXT_SHAPE)
⋮----
class TestDataLoader
⋮----
def test_load_normal_data(self, get_h5saver, init_data_to_export)
⋮----
data_loader = DataLoader(h5saver)
⋮----
data_loaded = data_loader.load_data(h5saver.get_node('/RawData/MyDet/Data2D/CH00/Data00'))
⋮----
def test_load_one_node(self, get_h5saver, init_data_to_export)
⋮----
data_loaded = data_loader.load_data(h5saver.get_node('/RawData/MyDet/Data2D/CH00/Data01'), load_all=True)
⋮----
def test_load_normal_data_with_bkg(self, get_h5saver, init_data_to_export)
⋮----
data_loaded = data_loader.load_data(h5saver.get_node('/RawData/MyDet/Data2D/CH00/Data00'), with_bkg=True)
⋮----
def test_load_enlargeable_data(self, get_h5saver, init_data_to_export)
⋮----
Nadd_data = 3
⋮----
nav_axis_node = h5saver.get_node('/RawData/MyDet/NavAxes/Axis00')
⋮----
data_loaded = data_loader.load_data('/RawData/MyDet/Data2D/CH00/EnlData00')
⋮----
def test_load_all(self, get_h5saver, init_data_to_export)
⋮----
data_all = DataToExport('All')
⋮----
def test_load_data_from_axis_node(self, get_h5saver)
⋮----
"""This is what happens in the h5browser to display axes in the viewers"""
⋮----
dwa = data_loader.load_data(axis_node.path)
⋮----
#missing handling units?
⋮----
assert dwa.axes[0].label == LABEL  # should not be that as the retrieved axis label of an
# axis node from this type of loading should be 'index'
assert dwa.axes[0].units == UNITS  # should not be that as the retrieved axis units of an
# axis node from this type of loading should be ''
</file>

<file path="tests/h5module_test/exporter_test.py">
LABEL = 'A Label'
UNITS = 'units'
OFFSET = -20.4
SCALING = 0.22
SIZE = 20
DATA = OFFSET + SCALING * np.linspace(0, SIZE-1, SIZE)
⋮----
DATA0D = np.array([2.7])
DATA1D = np.arange(0, 10)
DATA2D = np.arange(0, 5*6).reshape((5, 6))
DATAND = np.arange(0, 5 * 6 * 3).reshape((5, 6, 3))
Nn0 = 10
Nn1 = 5
⋮----
def init_axis(data=None, index=0)
⋮----
data = DATA
⋮----
data = DATA2D
dwa = data_mod.DataWithAxes(name, source, data=[data for ind in range(Ndata)],
⋮----
@pytest.fixture(scope="session")
def create_data(tmp_path_factory) -> Path
⋮----
fn = tmp_path_factory.mktemp("data") / 'mydata.h5'
h5saver = H5SaverLowLevel()
⋮----
datasaver = DataSaverLoader(h5saver)
⋮----
dwa = init_data()
⋮----
@pytest.fixture
def get_h5saver(create_data)
⋮----
path = create_data
⋮----
class TestH5Exporter
⋮----
def test_exporters_registry(self)
⋮----
factory = h5export.ExporterFactory()
⋮----
def test_register_exporter()
⋮----
exporter_modules = register_exporter('pymodaq_data.h5modules')
assert len(exporter_modules) >= 1  # this is the base exporter module
⋮----
def test_txt_exporter(get_h5saver, tmp_path)
⋮----
h5saver = get_h5saver
dataloader = DataSaverLoader(h5saver)
axis_loader = AxisSaverLoader(h5saver)
dwa = dataloader.load_data('/RawData/Data00')
⋮----
exporter = h5export.ExporterFactory.create_exporter('txt', 'Text files')
⋮----
#exporting 2D data as txt
file_path = tmp_path.joinpath('exported_data.txt')
⋮----
# exporting 1D data as txt
file_path = tmp_path.joinpath('exported_axis.txt')
⋮----
def test_npy_exporter(get_h5saver, tmp_path)
⋮----
axis = axis_loader.load_axis('/RawData/Axis00')
exporter = h5export.ExporterFactory.create_exporter('npy', 'Binary NumPy format')
⋮----
#exporting 2D data as npy
file_path = tmp_path.joinpath('exported_data.npy')
⋮----
# exporting 1D data as npy
file_path = tmp_path.joinpath('exported_axis.npy')
</file>

<file path="tests/h5module_test/saving_test.py">
# -*- coding: utf-8 -*-
"""
Created the 21/11/2022

@author: Sebastien Weber
"""
⋮----
tested_backend = ['tables', 'h5py']  # , 'h5pyd']
⋮----
@pytest.fixture()
def get_h5saver_lowlevel(tmp_path)
⋮----
h5saver = saving.H5SaverLowLevel()
addhoc_file_path = tmp_path.joinpath('h5file.h5')
⋮----
@pytest.fixture(scope="module")
def session_path(tmp_path_factory)
⋮----
def generate_random_data(shape, dtype=float)
⋮----
class TestH5SaverLowLevel
⋮----
def test_init_file(self, tmp_path)
⋮----
metadata = dict(attr1='attr1', attr2=(10, 2))
⋮----
def test_logger(self, get_h5saver_lowlevel)
⋮----
h5saver = get_h5saver_lowlevel
⋮----
LOGS = ['This', 'is', 'a', 'message']
⋮----
logger_array = h5saver.get_set_logger()
⋮----
def test_add_string_array(self, get_h5saver_lowlevel)
⋮----
#todo
⋮----
def test_add_array(self, get_h5saver_lowlevel)
⋮----
#"todo
⋮----
def test_incremental_group(self, get_h5saver_lowlevel)
⋮----
# "todo
</file>

<file path="tests/data_test.py">
# -*- coding: utf-8 -*-
"""
Created the 28/10/2022

@author: Sebastien Weber
"""
⋮----
data_processors = DataProcessorFactory()
⋮----
LABEL = 'A Label'
UNITS = 'mm'
OFFSET = -20.4
SCALING = 1.33
SIZE = 1024
DATA = OFFSET + SCALING * np.linspace(0, SIZE-1, SIZE)
⋮----
DATA0D = np.array([2.7])
DATA1D = np.arange(0, 10)
DATA2D = np.arange(0, 5*6).reshape((5, 6))
XAXIS_GAUSSIAN = np.linspace(0, 128, endpoint=False)
X0 = 100
DX = 20
YAXIS_GAUSSIAN = np.linspace(0, 128, endpoint=False)
Y0 = 79
DY = 12
GAUSSIAN_2D = mutils.gauss2D(YAXIS_GAUSSIAN, Y0, DY,
DATAND = np.arange(0, 5 * 6 * 3).reshape((5, 6, 3))
REAL_UNITS = 'm'
Nn0 = 10
Nn1 = 5
⋮----
DWA_RAW = data_mod.DataRaw('raw', units='s', data=[DATA1D, DATA1D])
⋮----
def init_axis(data=None, index=0, label=LABEL) -> data_mod.Axis
⋮----
data = DATA
⋮----
data = DATA2D
⋮----
def init_dataND()
⋮----
N0 = 5
N1 = 6
N2 = 3
DATAND = np.arange(0, N0 * N1 * N2).reshape((N0, N1, N2))
⋮----
axis0 = data_mod.Axis(label='myaxis0', data=np.linspace(0, N0 - 1, N0), index=0)
axis1 = data_mod.Axis(label='myaxis1', data=np.linspace(0, N1 - 1, N1), index=1)
axis2 = data_mod.Axis(label='myaxis2', data=np.linspace(0, N2 - 1, N2), index=2)
⋮----
@pytest.fixture()
def init_data_spread()
⋮----
Nspread = 21
sig_axis = data_mod.Axis(label='signal', index=1, data=np.linspace(0, DATA1D.size - 1, DATA1D.size))
nav_axis_0 = data_mod.Axis(label='nav0', index=0, data=np.random.rand(Nspread), spread_order=0)
nav_axis_1 = data_mod.Axis(label='nav1', index=0, data=np.random.rand(Nspread), spread_order=1)
⋮----
data_array = np.array([ind / Nspread * DATA1D for ind in range(Nspread)])
data = data_mod.DataRaw('mydata', distribution='spread', data=[data_array], nav_indexes=(0,),
⋮----
@pytest.fixture()
def init_spread_data_arrays()
⋮----
return ([np.random.randn(5), np.random.randn(5)],  # 0D spread arrays
[np.random.randn(5, 4), np.random.randn(5, 4)],  # 1D spread arrays
[np.random.randn(5, 2, 4),np.random.randn(5, 2, 4)],  # 2D spread arrays
⋮----
@pytest.fixture()
def init_data_uniform() -> data_mod.DataWithAxes
⋮----
nav_axis_0 = data_mod.Axis(label='nav0', index=0, data=np.linspace(0, Nn0 - 1, Nn0))
nav_axis_1 = data_mod.Axis(label='nav1', index=1, data=np.linspace(0, Nn1 - 1, Nn1))
sig_axis_0 = data_mod.Axis(label='signal0', index=2,
sig_axis_1 = data_mod.Axis(label='signal1', index=3,
⋮----
data_array = np.ones((Nn0, Nn1, DATA2D.shape[0], DATA2D.shape[1]))
data = data_mod.DataRaw('mydata', data=[data_array], nav_indexes=(0, 1),
⋮----
@pytest.fixture()
def init_axis_fixt()
⋮----
@pytest.fixture()
def init_data_fixt()
⋮----
@pytest.fixture()
def ini_data_to_export()
⋮----
dat1 = init_data(data=DATA2D, Ndata=2, name='data2D')
dat2 = init_data(data=DATA1D, Ndata=3, name='data1D')
data = data_mod.DataToExport(name='toexport', data=[dat1, dat2])
⋮----
class TestDataDim
⋮----
def test_index(self)
⋮----
dim = DataDim['Data0D']
⋮----
dim = DataDim['Data1D']
⋮----
dim = DataDim['Data2D']
⋮----
dim = DataDim['DataND']
⋮----
@pytest.mark.parametrize('dim1', ['Data0D', 'Data1D', 'Data2D', 'DataND'])
@pytest.mark.parametrize('dim2', ['Data0D', 'Data1D', 'Data2D', 'DataND'])
    def test_comparison(self, dim1: str, dim2: str)
⋮----
class TestAxis
⋮----
def test_units(self)
⋮----
NOT_A_STRING = 45
⋮----
axis = data_mod.Axis('axis', units=NOT_A_STRING, data=np.array([0, 1]))
⋮----
axis = data_mod.Axis('axis', units='unknown units', data=np.array([0, 1]))
⋮----
def test_units_as(self)
⋮----
eVs = np.array([1, 1.5, 2])
axis = data_mod.Axis('wavelength', units='eV', data=eVs)
⋮----
axis_again = data_mod.Axis('wavelength', units='eV', data=eVs)
new_axis = axis_again.units_as('nm', inplace=False, context='spectroscopy')
⋮----
axis_base = new_axis.to_base_units()
⋮----
def test_errors(self)
⋮----
def test_attributes(self, init_axis_fixt)
⋮----
ax = init_axis_fixt
⋮----
data_tmp = np.array([0.1, 2, 23, 44, 21, 20])  # non linear axis
ax = init_axis(data=data_tmp)
⋮----
def test_operation(self, init_axis_fixt)
⋮----
scale = 2
offset = 100
⋮----
ax_scaled = ax * scale
ax_offset = ax + offset
⋮----
def test_math(self, init_axis_fixt)
⋮----
def test_find_index(self, init_axis_fixt)
⋮----
def test_slice_getter(self, init_axis_fixt)
⋮----
ellipsis_axis = ax.iaxis[...]
ellipsis_axis_value = ax.vaxis[...]
⋮----
ind_start = 2
ind_end = 10
sliced_axis = ax.iaxis[ind_start:ind_end]
⋮----
sliced_axis_value = ax.vaxis[ax.get_data()[ind_start]:ax.get_data()[ind_end]]
⋮----
ind_int = 3
int_axis = ax.iaxis[ind_int]
int_axis_value = ax.vaxis[ax.get_data()[ind_int]]
⋮----
threshold = Q_(0.5, 'm')
⋮----
neg_index = 3
sliced_axis = ax.iaxis[ind_start:-neg_index]
⋮----
def test_slice_setter(self, init_axis_fixt)
⋮----
length = len(ax)
⋮----
axis_to_put_in = data_mod.Axis('replace', data=mutils.linspace_step(ind_start-10, ind_end-1-10, 1))
⋮----
def test_get_data(self)
⋮----
# spread axis
DATA = np.array([0, 1, 6, 8, 9])
axis = init_axis(DATA)
⋮----
# linear axis
DATA = np.array([1, 2, 3, 4])
⋮----
def test_get_data_at(self)
⋮----
INDEX = 2
INDEXES = (0, 3, 4)
INDEXES_array = np.array(INDEXES)
SLICE = slice(0, None, 2)
⋮----
class TestDataLowLevel
⋮----
def test_init(self)
⋮----
data = data_mod.DataLowLevel('myData')
⋮----
def test_timestamp(self)
⋮----
t1 = time.time()
⋮----
t2 = time.time()
⋮----
class TestDataBase
⋮----
Ndata = 2
data = data_mod.DataBase('myData', source=data_mod.DataSource(0), data=[DATA2D for ind in range(Ndata)])
⋮----
def test_labels(self)
⋮----
Ndata = 3
labels = ['label0', 'label1']
labels_auto = labels + ['CH02']
⋮----
data = data_mod.DataBase('myData',
⋮----
def test_data_source(self)
⋮----
data = data_mod.DataBase('myData', source='calculated',  data=[DATA2D])
⋮----
def test_get_dim(self, data_array, datadim)
⋮----
assert data.dim == data_mod.DataDim['Data0D']  # force dim to reflect datashape
⋮----
def test_force_dim(self)
⋮----
data = data_mod.DataBase('myData', source=data_mod.DataSource(0), data=DATA2D)  # only a ndarray
⋮----
data = data_mod.DataBase('myData', source=data_mod.DataSource(0), data=12.4)  # only a numeric
⋮----
data_mod.DataBase('myData', source=data_mod.DataSource(0), data=[DATA2D, DATA0D])  # list of different ndarray shape length
⋮----
data_mod.DataBase('myData', source=data_mod.DataSource(0), data=['12', 5])  # list of non ndarray
⋮----
def test_dunder(self)
⋮----
LENGTH = 24
data = init_data(DATA0D, Ndata=LENGTH)
⋮----
count = 0
⋮----
def test_comparison_units(self)
⋮----
dwa_mm = data_mod.DataRaw('dwa', units='mm', data=[np.array([0, 1, 2])])
dwa_um = data_mod.DataRaw('dwa', units='um', data=[np.array([0, 1, 2])])
dwa_um_equal = data_mod.DataRaw('dwa', units='um', data=[np.array([0, 1, 2]) * 1000])
dwa_unrelated = data_mod.DataRaw('dwa', units='', data=[np.array([0, 1, 2])])
⋮----
def test_maths(self)
⋮----
data = init_data(data=DATA2D, Ndata=2)
data1 = init_data(data=DATA2D, Ndata=2)
⋮----
data_sum = data + data1
⋮----
data_diff = data - data1
⋮----
data_mult = data * 0.85
⋮----
data_div = data / 0.85
⋮----
def test_multiply(self)
⋮----
units = 'm'
data = init_data(data=DATA1D, Ndata=2, units=units)
units1 = 'ms'
data1 = init_data(data=DATA1D, Ndata=2, units=units1)
number = 24.5
array = DATA1D * 0.01
⋮----
data_number = data * number
⋮----
data_array = data * array
⋮----
data_data = data * data1
⋮----
def test_abs(self)
⋮----
data_p = init_data(data=DATA2D, Ndata=2)
data_m = init_data(data=-DATA2D, Ndata=2)
⋮----
def test_average(self)
⋮----
WEIGHT = 5
FRAC = 0.23
⋮----
data1 = init_data(data=-DATA2D, Ndata=2)
⋮----
def test_append(self)
⋮----
labels = [f'label{ind}' for ind in range(Ndata)]
Ndatabis = 3
label_bis = [f'labelbis{ind}' for ind in range(Ndatabis)]
data = init_data(data=DATA1D, Ndata=Ndata, labels=labels)
data_bis = init_data(data=DATA1D, Ndata=Ndatabis, labels=label_bis)
⋮----
class TestDataWithAxesUniform
⋮----
data = data_mod.DataWithAxes('myData', source=data_mod.DataSource(0),
⋮----
def test_axis(self)
⋮----
@mark.parametrize('index', [-1, 0, 1, 3])
    def test_axis_index(self, index)
⋮----
index = 0
data = init_data(DATA2D, 2, axes=[init_axis(np.zeros((10,)), index)])
⋮----
index = 1
data = init_data(DATA2D, 2, axes=[init_axis(np.zeros((DATA2D.shape[index],)), index)])
⋮----
def test_get_shape_from_index(self)
⋮----
def test_get_axis_from_dim(self)
⋮----
index1 = 1
axis = init_axis(np.zeros((DATA2D.shape[index1],)), index1)
data = init_data(DATA2D, 2, axes=[axis])
⋮----
index0 = 0
axis = data.get_axis_from_index(index0)[0]
⋮----
axis = data.get_axis_from_index(index0, create=True)[0]
⋮----
def test_deep_copy_with_new_data(self)
⋮----
#on nav_indexes
IND_TO_REMOVE = 1
new_data = data.deepcopy_with_new_data([np.squeeze(dat[:, 0, :]) for dat in data.data],
⋮----
# on sig_indexes
IND_TO_REMOVE = 2
new_data = data.deepcopy_with_new_data([np.squeeze(dat[:, :, 0]) for dat in data.data],
⋮----
# on nav_indexes both axes
IND_TO_REMOVE = [0, 1]
new_data = data.deepcopy_with_new_data([np.squeeze(dat[0, 0, :]) for dat in data.data],
⋮----
# on all axes
IND_TO_REMOVE = [0, 1, 2]
new_data = data.deepcopy_with_new_data([np.atleast_1d(np.mean(dat)) for dat in data.data],
⋮----
@pytest.mark.parametrize("IND_MEAN", [0, 1, 2])
    def test_mean(self, IND_MEAN)
⋮----
data_ini = data.deepcopy()
new_data = data.mean(IND_MEAN)
⋮----
def test_sorted_uniform_1D(self)
⋮----
data_arrays = [np.array([10, 11, 12, 13, 14]), np.array([20, 21, 22, 23, 24])]
axis_array = np.array([5, 2, 3, 4, 1])
sorted_index = np.argsort(axis_array)
data = data_mod.DataRaw('mydata', distribution='uniform',
⋮----
AXIS_INDEX = 0
sorted_data = data.sort_data(axis_index=AXIS_INDEX)
⋮----
def test_sorted_uniform_2D_not_inplace(self)
⋮----
data_arrays = [np.array([[10, 11, 12, 13, 14],
axis_0 = data_mod.Axis('axis_0', data=np.array([-10, -15, -20]), index=0)
axis_1 = data_mod.Axis('axis_1', data=np.array([-10, -20, -30, -40, -50]), index=1)
⋮----
sorted_index_0 = np.argsort(axis_0.get_data())
sorted_index_1 = np.argsort(axis_1.get_data())
⋮----
sorted_data_0 = data.sort_data(axis_index=0)
⋮----
sorted_data_1 = data.sort_data(axis_index=1)
⋮----
def test_sorted_uniform_2D_inplace(self)
⋮----
sorted_data_0 = data.sort_data(axis_index=0, inplace=True)
⋮----
def test_interp(self)
⋮----
data_arrays = [np.array([11, 12, 14, 15]), np.array([21, 22, 24, 25])]
data_arrays_expected = [np.array([11, 12, 13, 14, 15]), np.array([21, 22, 23, 24, 25])]
axis_array = np.array([1, 2, 4, 5])
new_axis_array = np.array([1, 2, 3, 4, 5])
⋮----
dwa = data_mod.DataRaw('mydata', distribution='uniform',
⋮----
dwa_interp = dwa.interp(new_axis_array)
⋮----
dwa = init_data(DATA0D)
⋮----
def test_ft_ift(self)
⋮----
omega0 = 5
time_axis = data_mod.Axis('time', 's', data=np.linspace(0, 10*2*np.pi, 2**10))
dwa = data_mod.DataRaw('sinus', data=[np.sin(omega0 * time_axis.get_data())], labels=['sinus'],
⋮----
dwa_fft = dwa.ft(0)
dwa_processed = data_processors.get('argmax').process(dwa_fft.abs())
⋮----
dwa_ift = dwa_fft.ift(0)
⋮----
def test_fit(self)
⋮----
OMEGA0 = 5
OFFSET = -4
AMPLITUDE = 2
PHI = 2 * np.pi /3
⋮----
OMEGA02 = 5
OFFSET2 = -4
AMPLITUDE2 = 2
PHI2 = 2 * np.pi /6
⋮----
time_axis = data_mod.Axis('time', 's', data=np.linspace(0, 2 * 2 * np.pi, 2 ** 8))
dwa = data_mod.DataRaw('sinus',
def my_sinus(x, a, offset, omega0, phi)
⋮----
dwa_fit = dwa.fit(my_sinus, initial_guess=(AMPLITUDE, OFFSET, OMEGA0, PHI))
⋮----
def test_find_peaks(self)
⋮----
PHI = np.pi /3
⋮----
OMEGA02 = 20
OFFSET2 = 2
AMPLITUDE2 = 1
PHI2 = np.pi /6
⋮----
time_axis = data_mod.Axis('time', 's', data=np.linspace(0, 10 * 2 * np.pi, 2 ** 10))
⋮----
dwa_ft = dwa.ft()
⋮----
dte_peak = dwa.ft().abs().find_peaks(height=50)
⋮----
class TestNavIndexes
⋮----
def test_set_nav_indexes(self)
⋮----
class TestDataWithAxesSpread
⋮----
def test_init_data(self, init_data_spread)
⋮----
dwa = data_mod.DataWithAxes(name='spread', source=data_mod.DataSource['raw'],
⋮----
assert dwa.inav[10].distribution.name == 'uniform'  # because the remangin signal data has uniform axes
⋮----
def test_nav_index(self, init_data_spread)
⋮----
def test_nav_axis_length(self, init_data_spread)
⋮----
def test_compute_shape(self, init_data_spread)
⋮----
def test_repr(self, init_data_spread)
⋮----
def test_deep_copy_with_new_data(self, init_data_spread)
⋮----
IND_TO_REMOVE = 0
new_data = data.deepcopy_with_new_data([np.squeeze(dat[0, :]) for dat in data.data],
⋮----
new_data = data.deepcopy_with_new_data([np.squeeze(dat[:, 0]) for dat in data.data],
⋮----
@pytest.mark.parametrize("IND_MEAN", [0, 1])
    def test_mean(self, IND_MEAN, init_data_spread)
⋮----
def test_sorted_spread_signal(self, init_spread_data_arrays)
⋮----
axis_0_0 = data_mod.Axis('axis_0_0', data=np.array([0, 4, 2, 45, -10]), index=0, spread_order=0)
axis_0_1 = data_mod.Axis('axis_0_1', data=np.array([-10, -20, -30, -40, -50]), index=0, spread_order=1)
⋮----
axes = [axis_0_0.copy(), axis_0_1.copy()]
⋮----
sorted_index_0_0 = np.argsort(axis_0_0.get_data())
⋮----
data = data_mod.DataRaw('mydata', distribution='spread',
⋮----
sorted_data = data.sort_data(axis_index=0, spread_index=0)
⋮----
dwa = data_mod.DataRaw('mydata', distribution='spread',
⋮----
class TestSlicingUniform
⋮----
def test_slice_navigation(self, init_data_uniform)
⋮----
data_raw = init_data_uniform
⋮----
data_00: data_mod.DataWithAxes = data_raw.inav[0, :]
data_OO_value: data_mod.DataWithAxes = data_raw.vnav[0., :]
⋮----
data_01 = data_raw.inav[:, 2]
axis = data_raw.get_axis_from_index(data_raw.nav_indexes[1])[0]
data_01_value = data_raw.vnav[:, axis.get_data()[2]]
⋮----
data_1 = data_raw.inav[0, 2]
axis_0 = axis = data_raw.get_axis_from_index(data_raw.nav_indexes[0])[0]
axis_1 = axis = data_raw.get_axis_from_index(data_raw.nav_indexes[1])[0]
data_1_value = data_raw.vnav[axis_0.get_data()[0],
⋮----
data_2: data_mod.DataWithAxes = data_raw.inav[0:3, 2:4]
data_2_value = data_raw.vnav[axis_0.get_data()[0]:axis_0.get_data()[3],
⋮----
def test_slice_ellipsis(self, init_data_uniform)
⋮----
Nn1 = 7
Nn2 = 5
Nn3 = 4
⋮----
axis_0 = data_mod.Axis(label='nav0', index=0, data=np.linspace(0, Nn0 - 1, Nn0))
axis_1 = data_mod.Axis(label='nav1', index=1, data=np.linspace(0, Nn1 - 1, Nn1))
axis_2 = data_mod.Axis(label='nav3', index=2, data=np.linspace(0, Nn2 - 1, Nn2))
axis_3 = data_mod.Axis(label='signal1', index=3, data=np.linspace(0, Nn3 - 1, Nn3))
⋮----
data_array = np.ones((Nn0, Nn1, Nn2, Nn3))
data_raw = data_mod.DataRaw('mydata', data=[data_array], nav_indexes=(0, 1),
⋮----
data_sliced = data_raw.inav[0, ...]
data_sliced_value = data_raw.vnav[0, ...]
⋮----
data_sliced = data_raw.inav[..., 0]
⋮----
def test_slice_miscellanous(self, init_data_uniform)
⋮----
data_sliced = data_raw.inav[0:1, ...]
⋮----
def test_slice_signal(self, init_data_uniform)
⋮----
Ndata0 = DATA2D.shape[0]
Ndata1 = DATA2D.shape[1]
⋮----
data_00: data_mod.DataWithAxes = data_raw.isig[0, :]
⋮----
data_01 = data_raw.isig[:, 2]
⋮----
data_1 = data_raw.isig[0, 2]
⋮----
isig2_min = 0
isig3_min = 2
isig2_max = 3
isig3_max = 4
⋮----
data_2: data_mod.DataWithAxes = data_raw.isig[isig2_min:isig2_max, isig3_min:isig3_max]
⋮----
axis_2_values = data_raw.get_axis_from_index(data_raw.sig_indexes[0])[0].get_data()[isig2_min:isig2_max+1]
axis_3_values = data_raw.get_axis_from_index(data_raw.sig_indexes[1])[0].get_data()[isig3_min:isig3_max+1]
⋮----
data_2_vsliced = data_raw.vsig[min(axis_2_values):max(axis_2_values),
⋮----
data_3: data_mod.DataWithAxes = data_raw.isig[0:3, :-1]
⋮----
def test_slicing_setter(self)
⋮----
data_nav = data_mod.DataRaw('to replace', data=[np.ones((5, 6))])
⋮----
class TestSlicingSpread
⋮----
def test_slice_navigation(self, init_data_spread)
⋮----
data_0: data_mod.DataWithAxes = data.inav[3]
⋮----
data_1: data_mod.DataWithAxes = data.inav[3:6]
⋮----
def test_slice_signal(self, init_data_spread)
⋮----
data_0: data_mod.DataWithAxes = data.isig[3]
⋮----
data_1: data_mod.DataWithAxes = data.isig[3:6]
⋮----
class TestDataSource
⋮----
def test_data_raw(self)
⋮----
data = data_mod.DataRaw('myData', data=[DATA2D for ind in range(Ndata)])
⋮----
def test_data_calculated(self)
⋮----
data = data_mod.DataCalculated('myData', data=[DATA2D for ind in range(Ndata)])
⋮----
def test_data_from_roi(self)
⋮----
data = data_mod.DataFromRoi('myData', data=[DATA2D for ind in range(Ndata)])
⋮----
class TestDataToExport
⋮----
def test_init(self, ini_data_to_export)
⋮----
def test_data_type(self)
⋮----
def test_append(self, ini_data_to_export)
⋮----
dat3 = init_data(data=DATA0D, Ndata=1, name='data0D')
⋮----
def test_getitem(self)
⋮----
dat0D = init_data(DATA0D, 2, name='my0DData', source='raw')
dat1D_calculated = init_data(DATA1D, 2, name='my1DDatacalculated', source='calculated')
dat1D_raw = init_data(DATA1D, 2, name='my1DDataraw', source='raw')
⋮----
data = data_mod.DataToExport(name='toexport', data=[dat0D, dat1D_calculated, dat1D_raw])
⋮----
index_slice = 1
sliced_data = data[index_slice:]
⋮----
sliced_data = data[0:2]
⋮----
sliced_data = data[0::2]
⋮----
def test_get_data_from_source(self)
⋮----
def test_get_data_from_attribute(self)
⋮----
def test_get_data_from_missing_attribute(self)
⋮----
EXTRA_ATTRIBUE = 'aweirdattribute'
⋮----
def test_get_data_by_dim(self, ini_data_to_export)
⋮----
dat4 = init_data(data=DATA2D, Ndata=1, name='0_data2Dbis')
# '0_data2Dbis' 0 is meant for the name to show first if filtered data are sorted
⋮----
def test_get_data_from_name(self, ini_data_to_export)
⋮----
# without origin first
⋮----
dat3 = init_data(data=DATA2D, Ndata=1, name='data2Dbis')
⋮----
dat4 = init_data(data=DATA2D, Ndata=1, name='data2D')
⋮----
def test_get_full_names(self, ini_data_to_export)
⋮----
ORGIN1 = 'origin1'
ORIGIN2 = 'origin2'
⋮----
def test_get_origins(self, ini_data_to_export)
⋮----
ORIGIN1 = 'origin1'
⋮----
def test_get_data_from_name_origin(self, ini_data_to_export)
⋮----
# with origin
⋮----
def test_get_data_from_full_names(self, ini_data_to_export)
⋮----
def test_index(self, ini_data_to_export)
⋮----
dat3 = init_data(data=DATA2D, Ndata=2, name='data2D3')
dat4 = init_data(data=0.1*dat1.data[0], Ndata=2, name=dat1.name)
⋮----
assert dte.index(dat3) != 0  # same data but not the same name
assert dte.index(dat4) != 0  # same name but not the same data
⋮----
def test_index_from_name_origin(self, ini_data_to_export)
⋮----
def test_pop(self, ini_data_to_export)
⋮----
dat = data.pop(1)
⋮----
def test_remove(self, ini_data_to_export)
⋮----
dat2bis = data.remove(dat2)
⋮----
def test_get_names(self, ini_data_to_export)
⋮----
def test_math(self)
⋮----
dat1 = init_data(data=DATA2D, Ndata=2, name='data2D1')
dat2 = init_data(data=0.2 * DATA2D, Ndata=2, name='data2D2')
dat2bis = init_data(data=0.2 * DATA2D.reshape(DATA2D.shape[-1::-1]), Ndata=2, name='data2D2bis')
dat3 = init_data(data=-0.7 * DATA2D, Ndata=2, name='data2D3')
⋮----
data1 = data_mod.DataToExport(name='toexport', data=[dat1, dat2])
data2 = data_mod.DataToExport(name='toexport', data=[dat3, dat2])
data3 = data_mod.DataToExport(name='toexport', data=[dat3, dat2bis])
data4 = data_mod.DataToExport(name='toexport', data=[dat3, dat2, dat1])
⋮----
data_sum = data1 + data2
data_diff = data1 - data2
⋮----
MUL_COEFF = 0.24
DIV_COEFF = 12.7
⋮----
data_mul = data1 * MUL_COEFF
data_div = data2 / DIV_COEFF
⋮----
WEIGHT = 6
⋮----
data1 = data1.average(data2, WEIGHT)
⋮----
def test_merge(self)
⋮----
dat1 = init_data(data=DATA1D, Ndata=1, name='data1D1')
dat2 = init_data(data=0.2 * DATA1D, Ndata=2, name='data1D2')
dat3 = init_data(data=-0.7 * DATA1D, Ndata=3, name='data1D3')
⋮----
dte = data_mod.DataToExport('merging', data=[dat1, dat2, dat3])
⋮----
dwa = dte.merge_as_dwa('Data1D')
⋮----
class TestUnits
⋮----
def test_unit_in_registry(self)
⋮----
dwa = data_mod.DataRaw('data', units='unknown_unit', data=[np.array([0, 1, 2])])
assert dwa.units == ''  # transforms to dimensionless
⋮----
dwa = data_mod.DataRaw('data', units='ms', data=[np.array([0, 1, 2])])
⋮----
def test_add_with_units(self)
⋮----
array_1 = np.array([0, 1, 2])
dwa_1 = data_mod.DataRaw('data', units='s', data=[array_1])
⋮----
dwa_2 = data_mod.DataRaw('data', units='ms', data=[array_1])
⋮----
dwa_sum_s = dwa_1 + dwa_2
⋮----
dwa_sum_ms = dwa_2 + dwa_1
⋮----
array = np.array([0, 1, 2])
dwa_s = data_mod.DataRaw('data', units='s', data=[array])
⋮----
dwa_ms = dwa_s.units_as('ms')
⋮----
dwa_ms = dwa_s.units_as('ms', inplace=False)
⋮----
wavelength = np.array([400, 600, 800])
dwa = data_mod.DataRaw('data', units='nm', data=[wavelength])
dwa_fs = dwa.units_as('eV', inplace=False, context='spectroscopy')
⋮----
class TestNumpyUfunc
⋮----
def test_add(self, elt1, elt2, unit_error)
⋮----
dwa_add = np.add(elt1, elt2)
⋮----
elt1bis = elt1.data[ind]
⋮----
elt1bis = elt1.m_as(REAL_UNITS)
⋮----
elt1bis = elt1
⋮----
elt2bis = elt2.data[ind]
⋮----
elt2bis = elt2.m_as(REAL_UNITS)
⋮----
elt2bis = elt2
⋮----
@pytest.mark.parametrize('operator', (np.add, np.subtract))
    def test_add_subtract_operator(self, operator)
⋮----
dwa_s = data_mod.DataRaw('raw', units='s', data=[DATA1D, DATA1D])
dwa_ms = data_mod.DataRaw('raw', units='ms', data=[DATA1D, DATA1D])
dwa_m = data_mod.DataRaw('raw', units='m', data=[DATA1D, DATA1D])
⋮----
dwa_s_ms = operator(dwa_s, dwa_ms)
⋮----
def test_mult_operator(self)
⋮----
dwa_s_ms = np.multiply(dwa_s, dwa_ms)
⋮----
def test_quantity_dwa_mult(self)
⋮----
q = data_mod.Q_(3.5, 's')
dwa_s = data_mod.DataRaw('raw', units='Hz', data=[DATA1D, DATA1D])
⋮----
dwa_ufunc = q * dwa_s
⋮----
class TestFuncNumpy
⋮----
def test_arg_min_max(self)
⋮----
axis_array = np.linspace(-10, 10, 101, endpoint=True)
data_array = np.sin(2*np.pi / 4 * axis_array) * gauss1D(axis_array, 0, 5)
dwa_max_min = data_mod.DataRaw('raw', data=[data_array],
dwa_min = np.min(dwa_max_min)
dwa_arg_min = np.argmin(dwa_max_min)
dwa_max = np.max(dwa_max_min)
dwa_arg_max = np.argmax(dwa_max_min)
⋮----
def test_all(self)
⋮----
dwa_bool = data_mod.DataRaw('raw', units='', data=[DATA1D == DATA1D])
⋮----
def test_func_remove_axis(self)
⋮----
dwa = data_mod.DataRaw('raw', units='', data=[GAUSSIAN_2D],
⋮----
dwa_std = np.std(dwa, axis=0)
dwa_mean = np.mean(dwa, axis=0)
⋮----
dwa_max = np.max(dwa)
⋮----
dwa_min = np.min(dwa)
⋮----
dwa_min = np.min(dwa, axis=1)
⋮----
dwa_min = np.min(dwa, axis=0)
⋮----
dwa_min = np.min(dwa, axis=-1)
⋮----
dwa_min = np.min(dwa, axis=(0, 1))
⋮----
def test_booleans(self)
⋮----
dwa_bool = data_mod.DataRaw('raw', units='', data=[
⋮----
dwa_all = np.all(dwa_bool)
⋮----
dwa_all = np.all(dwa_bool, axis=0)
⋮----
dwa_all = np.all(dwa_bool, axis=1)
⋮----
dwa_any = np.any(dwa_bool, axis=0)
⋮----
def test_func_on_complex(self)
⋮----
ANGLE = np.pi / 3
dwa = data_mod.DataRaw('raw', units='', data=[np.exp(1j * ANGLE * np.ones((10,)))])
⋮----
dwa_angle_rad = np.angle(dwa)
⋮----
dwa_angle_deg = np.angle(dwa, deg=True)
⋮----
def test_db(self)
⋮----
dwa = data_mod.DataRaw('raw', units='s', data=[GAUSSIAN_2D],)
⋮----
dwa_db = dwa.to_dB()
⋮----
def test_all_close(self)
⋮----
dwa_a = data_mod.DataRaw('raw', units='', data=[DATA1D, DATA1D])
dwa_b = data_mod.DataRaw('raw', units='', data=[DATA1D, DATA1D])
⋮----
def test_flip_transpose_rotate(self)
⋮----
dwa = data_mod.DataRaw('raw', units='', data=[DATA2D])
⋮----
dwa_transform = np.flipud(dwa)
⋮----
dwa_transform = np.fliplr(dwa)
⋮----
dwa_transform = np.transpose(dwa)
⋮----
def test_roll(self)
⋮----
SHIFT = (10, 5)
dwa_rolled = np.roll(dwa, shift = SHIFT)
</file>

<file path="tests/processeor_test.py">
processors = DataProcessorFactory()
⋮----
config_processors = {
⋮----
# test 2D signals
Nsigx = 200
Nsigy = 100
Nnav = 10
x = np.linspace(-Nsigx / 2, Nsigx / 2 - 1, Nsigx)
y = np.linspace(-Nsigy / 2, Nsigy / 2 - 1, Nsigy)
⋮----
dat = np.zeros((Nnav, Nsigy, Nsigx))
⋮----
data = DataRaw('mydata', data=[dat], nav_indexes=(0,),
new_data = processors.get('sum', **config_processors).operate(data.isig[25:75, 75:125])
</file>

<file path="tests/serializer_test.py">
ser_factory = SerializableFactory()
⋮----
LABEL = 'A Label'
UNITS = 'mm'
OFFSET = -20.4
SCALING = 0.22
SIZE = 20
DATA = OFFSET + SCALING * np.linspace(0, SIZE-1, SIZE)
⋮----
DATA0D = np.array([2.7])
DATA1D = np.arange(0, 10)
DATA2D = np.arange(0, 5*6).reshape((5, 6))
DATAND = np.arange(0, 5 * 6 * 3).reshape((5, 6, 3))
Nn0 = 10
Nn1 = 5
⋮----
class DataFromPlugins(data_mod.DataRaw)
⋮----
""" Inheriting class for test purpose"""
⋮----
def init_axis(data=None, index=0)
⋮----
data = DATA
⋮----
data = DATA2D
⋮----
errors = [np.random.random_sample(data.shape) for _ in range(Ndata)]
⋮----
errors = None
⋮----
@pytest.fixture()
def get_data()
⋮----
dat0D = init_data(DATA0D, 2, name='my0DData', source='raw', errors=True)
dat1D_calculated = init_data(DATA1D, 2, name='my1DDatacalculated',
dat1D_plugin = init_data(DATA1D, 2, name='my1DDataraw', klass=DataFromPlugins,
dte = data_mod.DataToExport(name='toexport', data=[dat0D, dat1D_calculated, dat1D_plugin])
⋮----
def test_axis_serialization_deserialization()
⋮----
axis = init_axis()
⋮----
ser = axis.serialize(axis)
⋮----
axis_deser = axis.deserialize(ser)[0]
⋮----
axis_back = ser_factory.get_apply_deserializer(ser_factory.get_apply_serializer(axis))
⋮----
def test_dwa_serialization_deserialization(get_data)
⋮----
dte = get_data
⋮----
ser = ser_factory.get_apply_serializer(dwa)
⋮----
dwa_back = ser_factory.get_apply_deserializer(ser)
⋮----
def test_dte_serialization(get_data)
⋮----
ser = ser_factory.get_apply_serializer(dte)
⋮----
dte_back = ser_factory.get_apply_deserializer(ser)
⋮----
dte_back_factory = ser_factory.get_apply_deserializer(ser_factory.get_apply_serializer(dte))
</file>

<file path=".gitattributes">
# Auto detect text files and perform LF normalization
* text=auto
</file>

<file path=".gitignore">
# Compiled python modules.
*.pyc

_version.py

# Byte-compiled / optimized / DLL files
__pycache__/


*.py[cod]
*$py.class

# C extensions
*.so
.idea/*

# Distribution / packaging
.Python
documentation/_*
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
.hypothesis/
.pytest_cache/

*.idea

# Translations
*.mo
*.pot


# Django stuff:
*.log
local_settings.py
db.sqlite3

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
target/

# Jupyter Notebook
.ipynb_checkpoints

# pyenv
.python-version

# celery beat schedule file
celerybeat-schedule

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# VS code project settings
.vscode/*

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.idea/workspace.xml

*.iml


.tox
*.h5
!data.h5

*yacctab.py
*lextab.py
</file>

<file path=".mu_repo">
repo=.
repo=../pymodaq_utils
serial=True
</file>

<file path="CITATION.cff">
cff-version: 1.2.0
message: "If you use this software, please cite it as below."
authors:
- family-names: "Weber"
  given-names: "Sébastien J."
  orcid: "https://orcid.org/0000-0001-8531-5551"
title: "PyMoDAQ: An open-source Python-based software for modular data acquisition"
version: 4.2.4
doi: 10.1063/5.0032116
date-released: 2024-07-15
url: "https://github.com/PyMoDAQ/PyMoDAQ"
preferred-citation:
  type: article
  authors:
  - family-names: "Weber"
    given-names: "Sébastien J."
    orcid: "https://orcid.org/0000-0001-8531-5551"
  doi: "10.1063/5.0032116"
  journal: "Review of Scientific Instruments"
  month: 4
  start: 045104 # First page number
  end: 11 # Last page number
  title: "PyMoDAQ: An open-source Python-based software for modular data acquisition"
  issue: 4
  volume: 92
  year: 2021
</file>

<file path="LICENSE">
The MIT License (MIT)

Copyright (c) 2021 Sebastien Weber <sebastien.weber@cemes.fr>

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
</file>

<file path="MANIFEST.in">
graft src
include README.rst
include LICENSE
include src/pymodaq/ressources/VERSION
recursive-include src/pymodaq/ressources *.toml *.xml
recursive-include src/pymodaq *.png *.ico
recursive-exclude src/pymodaq *.pyc
</file>

<file path="pyproject.toml">
[build-system]
requires = ["hatchling>=1.9.0", "hatch-vcs"]
build-backend = "hatchling.build"

[project]
name = "pymodaq_data"
dynamic = [
    "version",
]
description = "Modular Data Acquisition with Python"
readme = "README.rst"
license = { file="LICENSE" }
requires-python = ">=3.8"
authors = [
    { name = "Sébastien Weber", email = "sebastien.weber@cemes.fr" },
]
classifiers = [
    "Development Status :: 5 - Production/Stable",
    "Environment :: Other Environment",
    "Intended Audience :: Science/Research",
    "License :: OSI Approved :: MIT License",
    "Natural Language :: English",
    "Operating System :: OS Independent",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Topic :: Scientific/Engineering :: Human Machine Interfaces",
    "Topic :: Scientific/Engineering :: Visualization",
    "Topic :: Software Development :: Libraries :: Python Modules",
    "Topic :: Software Development :: User Interfaces",
]
dependencies = [
    "pymodaq_utils>=0.0.8",
    "multipledispatch",
    "numpy < 2.0.0",
    "packaging",
    "pint",
    "python-dateutil",
    "scipy",
    "toml",
    "tables>=3.10; python_version>=\"3.10\"",  # issue with some version of required package blosc2>=2.2.8
    "tables<3.9; python_version<\"3.10\"",  # issue with some version of required package blosc2>=2.2.8
]



[project.optional-dependencies]
dev = [
    "hatch", 
    "flake8",
    "h5py",
    "pytest",
    "pytest-cov",
    "pytest-xdist",
]


[project.urls]
Homepage = "http://pymodaq.cnrs.fr"
Source = "https://github.com/PyMoDAQ/pymodaq_data"
Tracker = "https://github.com/PyMoDAQ/pymodaq_data/issues"

[tool.hatch.version]
source = "vcs"
fallback-version = "5.0.12"

[tool.hatch.build.targets.sdist]
include = [
    "/src",
]

[tool.hatch.build.hooks.vcs]
version-file = "_version.py"
</file>

<file path="README.rst">
PyMoDAQ Data
############

.. image:: https://img.shields.io/pypi/v/pymodaq_data.svg
   :target: https://pypi.org/project/pymodaq_data/
   :alt: Latest Version

.. image:: https://readthedocs.org/projects/pymodaq/badge/?version=latest
   :target: https://pymodaq.readthedocs.io/en/stable/?badge=latest
   :alt: Documentation Status

.. image:: https://codecov.io/gh/PyMoDAQ/pymodaq_data/branch/5.0.x_dev/graph/badge.svg?token=H32JflMEYR 
 :target: https://codecov.io/gh/PyMoDAQ/pymodaq_data

+-------------+-------------+---------------+
|             | Linux       | Windows       |
+=============+=============+===============+
| Python 3.9  | |39-linux|  | |39-windows|  |
+-------------+-------------+---------------+
| Python 3.10 | |310-linux| | |310-windows| |
+-------------+-------------+---------------+
| Python 3.11 | |311-linux| | |311-windows| |
+-------------+-------------+---------------+
| Python 3.12 | |312-linux| | |312-windows| |
+-------------+-------------+---------------+




.. |39-linux| image:: https://raw.githubusercontent.com/PyMoDAQ/pymodaq_data/badges/5.0.x_dev/tests_Linux_3.9.svg
    :target: https://github.com/PyMoDAQ/pymodaq_data/actions/workflows/tests.yml

.. |310-linux| image:: https://raw.githubusercontent.com/PyMoDAQ/pymodaq_data/badges/5.0.x_dev/tests_Linux_3.10.svg
    :target: https://github.com/PyMoDAQ/pymodaq_data/actions/workflows/tests.yml

.. |311-linux| image:: https://raw.githubusercontent.com/PyMoDAQ/pymodaq_data/badges/5.0.x_dev/tests_Linux_3.11.svg
    :target: https://github.com/PyMoDAQ/pymodaq_data/actions/workflows/tests.yml

.. |312-linux| image:: https://raw.githubusercontent.com/PyMoDAQ/pymodaq_data/badges/5.0.x_dev/tests_Linux_3.12.svg
    :target: https://github.com/PyMoDAQ/pymodaq_data/actions/workflows/tests.yml

.. |39-windows| image:: https://raw.githubusercontent.com/PyMoDAQ/pymodaq_data/badges/5.0.x_dev/tests_Windows_3.9.svg
    :target: https://github.com/PyMoDAQ/pymodaq_data/actions/workflows/tests.yml

.. |310-windows| image:: https://raw.githubusercontent.com/PyMoDAQ/pymodaq_data/badges/5.0.x_dev/tests_Windows_3.10.svg
    :target: https://github.com/PyMoDAQ/pymodaq_data/actions/workflows/tests.yml

.. |311-windows| image:: https://raw.githubusercontent.com/PyMoDAQ/pymodaq_data/badges/5.0.x_dev/tests_Windows_3.11.svg
    :target: https://github.com/PyMoDAQ/pymodaq_data/actions/workflows/tests.yml

.. |312-windows| image:: https://raw.githubusercontent.com/PyMoDAQ/pymodaq_data/badges/5.0.x_dev/tests_Windows_3.12.svg
    :target: https://github.com/PyMoDAQ/pymodaq_data/actions/workflows/tests.yml




.. figure:: http://pymodaq.cnrs.fr/en/latest/_static/splash.png
   :alt: shortcut


PyMoDAQ__, Modular Data Acquisition with Python, is a set of **python** modules used to interface any kind of
experiments. It simplifies the interaction with detector and actuator hardware to go straight to the data acquisition
of interest.

__ https://pymodaq.readthedocs.io/en/stable/?badge=latest

`PyMoDAQ data`__ is a set of utilities (constants, methods and classes) that are used
for Data Management. It is heavily used with the PyMoDAQ framework but can also be used as a standalone
package for data management in another context.

__ https://pymodaq.cnrs.fr/en/latest/developer_folder/data_management.html

What are Data?
--------------

Data are objects with many characteristics able to properly describe real data taken on an experiment
or calculated from theory:


*  a type: float, int, ...
*  a dimensionality: Data0D, Data1D, Data2D and higher
*  units (dealt with the pint python package)
*  axes
*  actual data as numpy arrays
*  uncertainty/error bars
* ...


.. figure:: https://pymodaq.cnrs.fr/en/latest/_images/data.png

   What is PyMoDAQ's data?.

The `PyMoDAQ Data` package
--------------------------

Because of this variety, `PyMoDAQ Data` introduce a set of objects including metadata (for instance the time of
acquisition) and various methods and properties to manipulate
them during analysis for instance (getting name, slicing, concatenating...),
save them and plot them (given you installed one of the available backend: *matplotlib* or *Qt* (
through the `pymodaq_gui` package)

To learn more, check the documentation__.

__ https://pymodaq.cnrs.fr/en/latest/data_management.html


Published under the MIT FREE SOFTWARE LICENSE

GitHub repo: https://github.com/PyMoDAQ

Documentation: http://pymodaq.cnrs.fr/
</file>

<file path="readthedocs.yml">
# .readthedocs.yml
# Read the Docs configuration file
# See https://docs.readthedocs.io/en/stable/config-file/v2.html for details

# Required
version: 2

build:
  os: "ubuntu-22.04"
  tools:
    python: "3.8"


# Build documentation in the docs/ directory with Sphinx
sphinx:
  configuration: docs/src/conf.py


# Optionally build your docs in additional formats such as PDF and ePub
formats: all

# Optionally set the version of Python and requirements required to build your docs
python:
  install:
    - requirements: docs/requirements.txt
    - method: pip
      path: .
</file>

</files>
